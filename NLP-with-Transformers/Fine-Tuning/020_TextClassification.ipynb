{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Build a Text Classifier\n",
    "\n",
    "<img style=\"float: right;\" src=\"images/nemo/nemo-app-stack.png\" width=400>\n",
    "\n",
    "In this notebook, you'll build an application to classify medical disease abstracts into one of three categories: cancer diseases, neurological diseases and disorders, and \"other\" for anything else.\n",
    "You'll use [NVIDIA NeMo](https://developer.nvidia.com/nvidia-nemo) (Neural Modules) to quickly set up the problem from the command line. \n",
    "\n",
    "**[2.1 NeMo Overview](#2.1-NeMo-Overview)**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.1.1 NeMo Models](#2.1.1-NeMo-Models)<br>\n",
    "**[2.2 Text Classification from the Command Line](#2.2-Text-Classification-from-the-Command-Line)**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.2.1 Prepare the Data](#2.2.1-Prepare-the-Data)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.2.2 Configuration File](#2.2.2-Configuration-File)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.2.2.1 OmegaConf Tool](#2.2.2.1-OmegaConf-Tool)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.2.3 Hydra-Enabled Python Script](#2.2.3-Hydra-Enabled-Python-Script)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.2.4 Exercise: Run an Experiment](#2.2.4-Exercise:-Run-an-Experiment)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.2.5 Visualize the Results with TensorBoard](#2.2.5-Visualize-the-Results-with-TensorBoard)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.2.6 Exercise: Change the Language Model](#2.2.6-Exercise:-Change-the-Language-Model)<br>\n",
    "**[2.3 PyTorch Lightning Model and Trainer Workflow](#2.3-PyTorch-Lightning-Model-and-Trainer-Workflow)**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.3.1 Script Key Features](#2.3.1-Script-Key-Features)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.3.2 Model Training from Scratch](#2.3.2-Model-Training-from-Scratch)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.3.3 Exercise: Query the Model](#2.3.3-Exercise:-Query-the-Model)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2.1 NeMo Overview\n",
    "NeMo is an open source toolkit for building conversational AI applications. NeMo is built around [Neural Modules](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/core/core.html#neural-modules), conceptual blocks of neural networks that take typed inputs and produce typed outputs. Such modules typically represent data layers, encoders, decoders, language models, loss functions, or methods of combining activations. NeMo makes it easy to combine and re-use these building blocks while providing a level of semantic correctness checking via its neural type system.\n",
    "\n",
    "The NeMo deep learning framework is based on [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning), a PyTorch wrapper that organizes PyTorch code for neural network training.  PyTorch Lightning provides easy and high-performant multi-GPU/multi-node mixed precision training options. Creating a deep neural network project, or **experiment**, with PyTorch Lightning requires two main components:\n",
    "1. [LightningModule](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html)\n",
    "2. [Trainer](https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html)\n",
    "\n",
    "The _LightningModule_ is used to organize PyTorch code into computation, optimizers, and loops for training, validation, and test.  This abstraction makes deep learning experiments easier to understand and reproduce. \n",
    "\n",
    "The _Trainer_ is then able to take the LightningModule and automate everything needed for deep learning training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 NeMo Models\n",
    "\n",
    "[NeMo models](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/core/core.html) are LightningModules that come equipped with all supporting infrastructure for training and reproducibility. This includes the deep learning model architecture, data preprocessing, optimizer, checkpoints, and experiment logging. NeMo models, like LightningModules, are also PyTorch modules and are fully compatible with the broader PyTorch ecosystem. Any NeMo model can be taken and plugged into any PyTorch workflow.  \n",
    "\n",
    "**Every NeMo model has an example configuration file and training script that can be found in the [NVIDIA NeMo GitHub Repo](https://github.com/NVIDIA/NeMo/tree/main/examples).**\n",
    "\n",
    "For this class, we'll use a local repo copy included in this environment, based on the downloadable [NGC NeMo container](https://ngc.nvidia.com/catalog/containers/nvidia:nemo), and focus on NLP models.  Execute the following cell to see a tree of NeMo models in the `examples/nlp` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mnemo/examples/nlp\u001b[00m\n",
      "├── \u001b[01;34mdialogue_state_tracking\u001b[00m\n",
      "│   ├── \u001b[01;34mconf\u001b[00m\n",
      "│   └── sgd_qa.py\n",
      "├── \u001b[01;34mentity_linking\u001b[00m\n",
      "│   ├── build_index.py\n",
      "│   ├── \u001b[01;34mconf\u001b[00m\n",
      "│   ├── \u001b[01;34mdata\u001b[00m\n",
      "│   ├── query_index.py\n",
      "│   └── self_alignment_pretraining.py\n",
      "├── \u001b[01;34mglue_benchmark\u001b[00m\n",
      "│   ├── glue_benchmark.py\n",
      "│   └── glue_benchmark_config.yaml\n",
      "├── \u001b[01;34minformation_retrieval\u001b[00m\n",
      "│   ├── bert_dpr.py\n",
      "│   ├── bert_joint_ir.py\n",
      "│   ├── \u001b[01;34mconf\u001b[00m\n",
      "│   ├── construct_random_negatives.py\n",
      "│   └── get_msmarco.sh\n",
      "├── \u001b[01;34mintent_slot_classification\u001b[00m\n",
      "│   ├── \u001b[01;34mconf\u001b[00m\n",
      "│   ├── \u001b[01;34mdata\u001b[00m\n",
      "│   └── intent_slot_classification.py\n",
      "├── \u001b[01;34mlanguage_modeling\u001b[00m\n",
      "│   ├── bert_pretraining.py\n",
      "│   ├── \u001b[01;34mconf\u001b[00m\n",
      "│   ├── convert_weights_to_nemo1.0.py\n",
      "│   ├── get_wkt2.sh\n",
      "│   └── transformer_lm.py\n",
      "├── \u001b[01;34mmachine_translation\u001b[00m\n",
      "│   ├── \u001b[01;34mconf\u001b[00m\n",
      "│   ├── create_tarred_monolingual_dataset.py\n",
      "│   ├── create_tarred_parallel_dataset.py\n",
      "│   ├── enc_dec_nmt.py\n",
      "│   ├── nmt_transformer_infer.py\n",
      "│   ├── \u001b[01;34mnmt_webapp\u001b[00m\n",
      "│   ├── preprocess_dataset.py\n",
      "│   └── translate_ddp.py\n",
      "├── \u001b[01;34mquestion_answering\u001b[00m\n",
      "│   ├── \u001b[01;34mconf\u001b[00m\n",
      "│   ├── \u001b[01;32mget_squad.py\u001b[00m\n",
      "│   └── question_answering_squad.py\n",
      "├── \u001b[01;34mtext2sparql\u001b[00m\n",
      "│   ├── \u001b[01;34mconf\u001b[00m\n",
      "│   ├── \u001b[01;34mdata\u001b[00m\n",
      "│   ├── evaluate_text2sparql.py\n",
      "│   └── text2sparql.py\n",
      "├── \u001b[01;34mtext_classification\u001b[00m\n",
      "│   ├── \u001b[01;34mconf\u001b[00m\n",
      "│   ├── \u001b[01;34mdata\u001b[00m\n",
      "│   ├── model_parallel_text_classification_evaluation.py\n",
      "│   └── text_classification_with_bert.py\n",
      "└── \u001b[01;34mtoken_classification\u001b[00m\n",
      "    ├── \u001b[01;34mconf\u001b[00m\n",
      "    ├── \u001b[01;34mdata\u001b[00m\n",
      "    ├── punctuation_capitalization_evaluate.py\n",
      "    ├── punctuation_capitalization_train.py\n",
      "    ├── token_classification_evaluate.py\n",
      "    └── token_classification_train.py\n",
      "\n",
      "27 directories, 31 files\n"
     ]
    }
   ],
   "source": [
    "!tree nemo/examples/nlp -L 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of models listed covering several classic NLP tasks.  We will focus on [text classification](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/text_classification.html) in this notebook and [token classification](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/token_classification.html) in the next notebook on named entity recognition (NER). \n",
    "\n",
    "Notice that each NeMo model type includes a `conf` folder for configuration files and at least one Python training script file.  \n",
    "\n",
    "Execute the following cell to see more detail for the text classification model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/dli/task/nemo/examples/nlp/text_classification\u001b[00m\n",
      "├── \u001b[01;34mconf\u001b[00m\n",
      "│   └── text_classification_config.yaml\n",
      "├── \u001b[01;34mdata\u001b[00m\n",
      "│   └── import_datasets.py\n",
      "├── model_parallel_text_classification_evaluation.py\n",
      "└── text_classification_with_bert.py\n",
      "\n",
      "2 directories, 4 files\n"
     ]
    }
   ],
   "source": [
    "TC_DIR = \"/dli/task/nemo/examples/nlp/text_classification\"\n",
    "!tree $TC_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The config file, `text_classification_config.yaml`, specifies model, training, and experiment management details, such as file locations, pretrained models, and hyperparameters.\n",
    "\n",
    "The Python script, `text_classification_with_bert.py`, encapsulates everything you need to run a text classification experiment defined by the configuration file.  It employs Facebook's [Hydra](https://hydra.cc/) tool for configuration management, which allows you to run the entire experiment just with the script, using command line options to override the config values!\n",
    "\n",
    "The key to building an experiment quickly, is to  understand what the default config file includes, and what needs to be changed for your own project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2.2 Text Classification from the Command Line\n",
    "The question we want to answer is: \n",
    "\n",
    "**Given a medical disease abstract, is the abstract about cancer, a neurological disorder, or something else?**\n",
    "\n",
    "This is a 3-class text classification problem.  We'll use the NeMo [text classification model](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/text_classification.html) with three classes (labels): \"cancer\" (0), \"neurological disorders\" (1), and \"other\" (2).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.1 Prepare the Data\n",
    "You've already explored the [NCBI-disease corpus](https://www.ncbi.nlm.nih.gov/CBBresearch/Dogan/DISEASE/) and the text classification dataset derived from it in the [Explore the Data](010_ExploreData.ipynb) notebook.  Recall that the text classification files consist of tab-delimited abstracts and labels, with a row of headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dli/task/data/NCBI_tc-3/dev.tsv   /dli/task/data/NCBI_tc-3/train.tsv\n",
      "/dli/task/data/NCBI_tc-3/test.tsv\n"
     ]
    }
   ],
   "source": [
    "TC3_DATA_DIR = '/dli/task/data/NCBI_tc-3'\n",
    "!ls $TC3_DATA_DIR/*.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****\n",
      "train.tsv sample\n",
      "*****\n",
      "sentence\tlabel\n",
      "Identification of APC2, a homologue of the adenomatous polyposis coli tumour suppressor . The adenomatous polyposis coli ( APC ) tumour-suppressor protein controls the Wnt signalling pathway by forming a complex with glycogen synthase kinase 3beta ( GSK-3beta ) , axin / conductin and betacatenin . Complex formation induces the rapid degradation of betacatenin . In colon carcinoma cells , loss of APC leads to the accumulation of betacatenin in the nucleus , where it binds to and activates the Tcf-4 transcription factor ( reviewed in [ 1 ] [ 2 ] ) . Here , we report the identification and genomic structure of APC homologues . Mammalian APC2 , which closely resembles APC in overall domain structure , was functionally analyzed and shown to contain two SAMP domains , both of which are required for binding to conductin . Like APC , APC2 regulates the formation of active betacatenin-Tcf complexes , as demonstrated using transient transcriptional activation assays in APC - / - colon carcinoma cells . Human APC2 maps to chromosome 19p13 . 3 . APC and APC2 may therefore have comparable functions in development and cancer .\t0\n",
      "A common MSH2 mutation in English and North American HNPCC families: origin, phenotypic expression, and sex specific differences in colorectal cancer . The frequency , origin , and phenotypic expression of a germline MSH2 gene mutation previously identified in seven kindreds with hereditary non-polyposis cancer syndrome (HNPCC) was investigated . The mutation ( A-- > T at nt943 + 3 ) disrupts the 3 splice site of exon 5 leading to the deletion of this exon from MSH2 mRNA and represents the only frequent MSH2 mutation so far reported . Although this mutation was initially detected in four of 33 colorectal cancer families analysed from eastern England , more extensive analysis has reduced the frequency to four of 52 ( 8 % ) English HNPCC kindreds analysed . In contrast , the MSH2 mutation was identified in 10 of 20 ( 50 % ) separately identified colorectal families from Newfoundland . To investigate the origin of this mutation in colorectal cancer families from England ( n = 4 ) , Newfoundland ( n = 10 ) , and the United States ( n = 3 ) , haplotype analysis using microsatellite markers linked to MSH2 was performed . Within the English and US families there was little evidence for a recent common origin of the MSH2 splice site mutation in most families . In contrast , a common haplotype was identified at the two flanking markers ( CA5 and D2S288 ) in eight of the Newfoundland families . These findings suggested a founder effect within Newfoundland similar to that reported by others for two MLH1 mutations in Finnish HNPCC families . We calculated age related risks of all , colorectal , endometrial , and ovarian cancers in nt943 + 3 A-- > T MSH2 mutation carriers ( n = 76 ) for all patients and for men and women separately . For both sexes combined , the penetrances at age 60 years for all cancers  and for colorectal cancer were 0 . 86 and 0 . 57 , respectively . The risk of colorectal cancer was significantly higher ( p < 0 . 01 ) in males than females ( 0 . 63 v 0 . 30 and 0 . 84 v 0 . 44 at ages 50 and 60 years , respectively ) . For females there was a high risk of endometrial cancer ( 0 . 5 at age 60 years ) and premenopausal ovarian cancer ( 0 . 2 at 50 years ) . These intersex differences in colorectal cancer risks have implications for screening programmes and for attempts to identify colorectal cancer susceptibility modifiers .\t0\n",
      "\n",
      "\n",
      "*****\n",
      "dev.tsv sample\n",
      "*****\n",
      "sentence\tlabel\n",
      "BRCA1 is secreted and exhibits properties of a granin. Germline mutations in BRCA1 are responsible for most cases of inherited breast and ovarian cancer . However , the function of the BRCA1 protein has remained elusive . We now show that BRCA1 encodes a 190-kD protein with sequence homology and biochemical analogy to the granin protein family . Interestingly , BRCA2 also includes a motif similar to the granin consensus at the C terminus of the protein . Both BRCA1 and the granins localize to secretory vesicles , are secreted by a regulated pathway , are post-translationally glycosylated and are responsive to hormones . As a regulated secretory protein , BRCA1 appears to function by a mechanism not previously described for tumour suppressor gene products . . \t0\n",
      "Ovarian cancer risk in BRCA1 carriers is modified by the HRAS1 variable number of tandem repeat (VNTR) locus. Women who carry a mutation in the BRCA1 gene ( on chromosome 17q21 ) , have an 80 % risk of breast cancer and a 40 % risk of ovarian cancer by the age of 70 ( ref . 1 ) . The variable penetrance of BRCA1 suggests that other genetic and non-genetic factors play a role in tumourigenesis in these individuals . The HRAS1 variable number of tandem repeats ( VNTR ) polymorphism , located 1 kilobase ( kb ) downstream of the HRAS1 proto-oncogene ( chromosome 11p15 . 5 ) is one possible genetic modifier of cancer penetrance . Individuals who have rare alleles of the VNTR have an increased risk of certain types of cancers , including breast cancer ( 2-4 ) . To investigate whether the presence of rare HRAS1 alleles increases susceptibility to hereditary breast and ovarian cancer , we have typed a panel of 307 female BRCA1 carriers at this locus using a PCR-based technique . The risk for ovarian cancer was 2 . 11 times greater for BRCA1 carriers harbouring one or two rare HRAS1 alleles , compared to carriers with only common alleles ( P = 0 . 015 ) . The magnitude of the relative risk associated with a rare HRAS1 allele was not altered by adjusting for the other known risk factors for hereditary ovarian cancer ( 5 ) . Susceptibility to breast cancer did not appear to be affected by the presence of rare HRAS1 alleles . This study is the first to show the effect of a modifying gene on the penetrance of an inherited cancer syndrome \t0\n",
      "\n",
      "\n",
      "*****\n",
      "test.tsv sample\n",
      "*****\n",
      "sentence\tlabel\n",
      "Clustering of missense mutations in the ataxia-telangiectasia gene in a sporadic T-cell leukaemia. Ataxia-telangiectasia ( A-T ) is a recessive multi-system disorder caused by mutations in the ATM gene at 11q22-q23 ( ref . 3 ) . The risk of cancer , especially lymphoid neoplasias , is substantially elevated in A-T patients and has long been associated with chromosomal instability . By analysing tumour DNA from patients with sporadic T-cell prolymphocytic leukaemia ( T-PLL ) , a rare clonal malignancy with similarities to a mature T-cell leukaemia seen in A-T , we demonstrate a high frequency of ATM mutations in T-PLL . In marked contrast to the ATM mutation pattern in A-T , the most frequent nucleotide changes in this leukaemia were missense mutations . These clustered in the region corresponding to the kinase domain , which is highly conserved in ATM-related proteins in mouse , yeast and Drosophila . The resulting amino-acid substitutions are predicted to interfere with ATP binding or substrate recognition . Two of seventeen mutated T-PLL samples had a previously reported A-T allele . In contrast , no mutations were detected in the p53 gene , suggesting that this tumour suppressor is not frequently altered in this leukaemia . Occasional missense mutations in ATM were also found in tumour DNA from patients with B-cell non-Hodgkins lymphomas ( B-NHL ) and a B-NHL cell line . The evidence of a significant proportion of loss-of-function mutations and a complete absence of the normal copy of ATM in the majority of mutated tumours establishes somatic inactivation of this gene in the pathogenesis of sporadic T-PLL and suggests that ATM acts as a tumour suppressor . As constitutional DNA was not available , a putative hereditary predisposition to T-PLL will require further investigation . . \t0\n",
      "Myotonic dystrophy protein kinase is involved in the modulation of the Ca2+ homeostasis in skeletal muscle cells. Myotonic dystrophy ( DM ) , the most prevalent muscular disorder in adults , is caused by ( CTG ) n-repeat expansion in a gene encoding a protein kinase ( DM protein kinase ; DMPK ) and involves changes in cytoarchitecture and ion homeostasis . To obtain clues to the normal biological role of DMPK in cellular ion homeostasis , we have compared the resting [ Ca2 + ] i , the amplitude and shape of depolarization-induced Ca2 + transients , and the content of ATP-driven ion pumps in cultured skeletal muscle cells of wild-type and DMPK [ - / - ] knockout mice . In vitro-differentiated DMPK [ - / - ] myotubes exhibit a higher resting [ Ca2 + ] i than do wild-type myotubes because of an altered open probability of voltage-dependent l-type Ca2 + and Na + channels . The mutant myotubes exhibit smaller and slower Ca2 + responses upon triggering by acetylcholine or high external K + . In addition , we observed that these Ca2 + transients partially result from an influx of extracellular Ca2 + through the l-type Ca2 + channel . Neither the content nor the activity of Na + / K + ATPase and sarcoplasmic reticulum Ca2 + -ATPase are affected by DMPK absence . In conclusion , our data suggest that DMPK is involved in modulating the initial events of excitation-contraction coupling in skeletal muscle . . \t1\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the tab separated data\n",
    "print(\"*****\\ntrain.tsv sample\\n*****\")\n",
    "!head -n 3 $TC3_DATA_DIR/train.tsv\n",
    "print(\"\\n\\n*****\\ndev.tsv sample\\n*****\")\n",
    "!head -n 3 $TC3_DATA_DIR/dev.tsv\n",
    "print(\"\\n\\n*****\\ntest.tsv sample\\n*****\")\n",
    "!head -n 3 $TC3_DATA_DIR/test.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Note a few features of the data:\n",
    "* The preprocessed data is already in the \n",
    "\n",
    "   ```\n",
    "   [WORD][SPACE][WORD][SPACE][WORD][TAB][LABEL]\n",
    "   ``` \n",
    "   format specified in the [documentation](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/text_classification.html).\n",
    "* There is a header row, \"sentence label\", that should be removed.\n",
    "* The text is quite long, so `max_seq_length` values will need to take this into account for training.\n",
    "\n",
    "Start by removing the header rows.  There are a number of ways to do this, but since it is a simple change we can use a bash stream editor (`sed`) command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed 1d $TC3_DATA_DIR/train.tsv > $TC3_DATA_DIR/train_nemo_format.tsv\n",
    "!sed 1d $TC3_DATA_DIR/dev.tsv > $TC3_DATA_DIR/dev_nemo_format.tsv\n",
    "!sed 1d $TC3_DATA_DIR/test.tsv > $TC3_DATA_DIR/test_nemo_format.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****\n",
      "train_nemo_format.tsv sample\n",
      "*****\n",
      "Identification of APC2, a homologue of the adenomatous polyposis coli tumour suppressor . The adenomatous polyposis coli ( APC ) tumour-suppressor protein controls the Wnt signalling pathway by forming a complex with glycogen synthase kinase 3beta ( GSK-3beta ) , axin / conductin and betacatenin . Complex formation induces the rapid degradation of betacatenin . In colon carcinoma cells , loss of APC leads to the accumulation of betacatenin in the nucleus , where it binds to and activates the Tcf-4 transcription factor ( reviewed in [ 1 ] [ 2 ] ) . Here , we report the identification and genomic structure of APC homologues . Mammalian APC2 , which closely resembles APC in overall domain structure , was functionally analyzed and shown to contain two SAMP domains , both of which are required for binding to conductin . Like APC , APC2 regulates the formation of active betacatenin-Tcf complexes , as demonstrated using transient transcriptional activation assays in APC - / - colon carcinoma cells . Human APC2 maps to chromosome 19p13 . 3 . APC and APC2 may therefore have comparable functions in development and cancer .\t0\n",
      "A common MSH2 mutation in English and North American HNPCC families: origin, phenotypic expression, and sex specific differences in colorectal cancer . The frequency , origin , and phenotypic expression of a germline MSH2 gene mutation previously identified in seven kindreds with hereditary non-polyposis cancer syndrome (HNPCC) was investigated . The mutation ( A-- > T at nt943 + 3 ) disrupts the 3 splice site of exon 5 leading to the deletion of this exon from MSH2 mRNA and represents the only frequent MSH2 mutation so far reported . Although this mutation was initially detected in four of 33 colorectal cancer families analysed from eastern England , more extensive analysis has reduced the frequency to four of 52 ( 8 % ) English HNPCC kindreds analysed . In contrast , the MSH2 mutation was identified in 10 of 20 ( 50 % ) separately identified colorectal families from Newfoundland . To investigate the origin of this mutation in colorectal cancer families from England ( n = 4 ) , Newfoundland ( n = 10 ) , and the United States ( n = 3 ) , haplotype analysis using microsatellite markers linked to MSH2 was performed . Within the English and US families there was little evidence for a recent common origin of the MSH2 splice site mutation in most families . In contrast , a common haplotype was identified at the two flanking markers ( CA5 and D2S288 ) in eight of the Newfoundland families . These findings suggested a founder effect within Newfoundland similar to that reported by others for two MLH1 mutations in Finnish HNPCC families . We calculated age related risks of all , colorectal , endometrial , and ovarian cancers in nt943 + 3 A-- > T MSH2 mutation carriers ( n = 76 ) for all patients and for men and women separately . For both sexes combined , the penetrances at age 60 years for all cancers  and for colorectal cancer were 0 . 86 and 0 . 57 , respectively . The risk of colorectal cancer was significantly higher ( p < 0 . 01 ) in males than females ( 0 . 63 v 0 . 30 and 0 . 84 v 0 . 44 at ages 50 and 60 years , respectively ) . For females there was a high risk of endometrial cancer ( 0 . 5 at age 60 years ) and premenopausal ovarian cancer ( 0 . 2 at 50 years ) . These intersex differences in colorectal cancer risks have implications for screening programmes and for attempts to identify colorectal cancer susceptibility modifiers .\t0\n",
      "Age of onset in Huntington disease : sex specific influence of apolipoprotein E genotype and normal CAG repeat length . Age of onset ( AO ) of Huntington disease ( HD) is known to be correlated with the length of an expanded CAG repeat in the HD gene . Apolipoprotein E ( APOE ) genotype , in turn , is known to influence AO in Alzheimer disease , rendering the APOE gene a likely candidate to affect AO in other neurological diseases too . We therefore determined APOE genotype and normal CAG repeat length in the HD gene for 138 HD patients who were previously analysed with respect to CAG repeat length . Genotyping for APOE was performed blind to clinical information . In addition to highlighting the effect of the normal repeat length upon AO in maternally inherited HD and in male patients , we show that the APOE epsilon2epsilon3 genotype is associated with significantly earlier AO in males than in females . Such a sex difference in AO was not apparent for any of the other APOE genotypes . Our findings suggest that subtle differences in the course of the neurodegeneration  in HD  may allow interacting genes to exert gender specific effects upon AO.\t1\n",
      "\n",
      "\n",
      "*****\n",
      "dev_nemo_format.tsv sample\n",
      "*****\n",
      "BRCA1 is secreted and exhibits properties of a granin. Germline mutations in BRCA1 are responsible for most cases of inherited breast and ovarian cancer . However , the function of the BRCA1 protein has remained elusive . We now show that BRCA1 encodes a 190-kD protein with sequence homology and biochemical analogy to the granin protein family . Interestingly , BRCA2 also includes a motif similar to the granin consensus at the C terminus of the protein . Both BRCA1 and the granins localize to secretory vesicles , are secreted by a regulated pathway , are post-translationally glycosylated and are responsive to hormones . As a regulated secretory protein , BRCA1 appears to function by a mechanism not previously described for tumour suppressor gene products . . \t0\n",
      "Ovarian cancer risk in BRCA1 carriers is modified by the HRAS1 variable number of tandem repeat (VNTR) locus. Women who carry a mutation in the BRCA1 gene ( on chromosome 17q21 ) , have an 80 % risk of breast cancer and a 40 % risk of ovarian cancer by the age of 70 ( ref . 1 ) . The variable penetrance of BRCA1 suggests that other genetic and non-genetic factors play a role in tumourigenesis in these individuals . The HRAS1 variable number of tandem repeats ( VNTR ) polymorphism , located 1 kilobase ( kb ) downstream of the HRAS1 proto-oncogene ( chromosome 11p15 . 5 ) is one possible genetic modifier of cancer penetrance . Individuals who have rare alleles of the VNTR have an increased risk of certain types of cancers , including breast cancer ( 2-4 ) . To investigate whether the presence of rare HRAS1 alleles increases susceptibility to hereditary breast and ovarian cancer , we have typed a panel of 307 female BRCA1 carriers at this locus using a PCR-based technique . The risk for ovarian cancer was 2 . 11 times greater for BRCA1 carriers harbouring one or two rare HRAS1 alleles , compared to carriers with only common alleles ( P = 0 . 015 ) . The magnitude of the relative risk associated with a rare HRAS1 allele was not altered by adjusting for the other known risk factors for hereditary ovarian cancer ( 5 ) . Susceptibility to breast cancer did not appear to be affected by the presence of rare HRAS1 alleles . This study is the first to show the effect of a modifying gene on the penetrance of an inherited cancer syndrome \t0\n",
      "A novel homeodomain-encoding gene is associated with a large CpG island interrupted by the myotonic dystrophy unstable (CTG)n repeat. Myotonic dystrophy ( DM ) is associated with a ( CTG ) n trinucleotide repeat expansion in the 3-untranslated region of a protein kinase-encoding gene , DMPK , which maps to chromosome 19q13 . 3 . Characterisation of the expression of this gene in patient tissues has thus far generated conflicting data on alterations in the steady state levels of DMPK mRNA , and on the final DMPK protein levels in the presence of the expansion . The DM region of chromosome 19 is gene rich , and it is possible that the repeat expansion may lead to dysfunction of a number of transcription units in the vicinity , perhaps as a consequence of chromatin disruption . We have searched for genes associated with a CpG island at the 3 end of DMPK . Sequencing of this region shows that the island extends over 3 . 5 kb and is interrupted by the ( CTG ) n repeat . Comparison of genomic sequences downstream ( centromeric ) of the repeat in human and mouse identified regions of significant homology . These correspond to exons of a gene predicted to encode a homeodomain protein . RT-PCR analysis shows that this gene , which we have called DM locus-associated homeodomain protein ( DMAHP ) , is expressed in a number of human tissues , including skeletal muscle , heart and brain .\t1\n",
      "\n",
      "\n",
      "*****\n",
      "test_nemo_format.tsv sample\n",
      "*****\n",
      "Clustering of missense mutations in the ataxia-telangiectasia gene in a sporadic T-cell leukaemia. Ataxia-telangiectasia ( A-T ) is a recessive multi-system disorder caused by mutations in the ATM gene at 11q22-q23 ( ref . 3 ) . The risk of cancer , especially lymphoid neoplasias , is substantially elevated in A-T patients and has long been associated with chromosomal instability . By analysing tumour DNA from patients with sporadic T-cell prolymphocytic leukaemia ( T-PLL ) , a rare clonal malignancy with similarities to a mature T-cell leukaemia seen in A-T , we demonstrate a high frequency of ATM mutations in T-PLL . In marked contrast to the ATM mutation pattern in A-T , the most frequent nucleotide changes in this leukaemia were missense mutations . These clustered in the region corresponding to the kinase domain , which is highly conserved in ATM-related proteins in mouse , yeast and Drosophila . The resulting amino-acid substitutions are predicted to interfere with ATP binding or substrate recognition . Two of seventeen mutated T-PLL samples had a previously reported A-T allele . In contrast , no mutations were detected in the p53 gene , suggesting that this tumour suppressor is not frequently altered in this leukaemia . Occasional missense mutations in ATM were also found in tumour DNA from patients with B-cell non-Hodgkins lymphomas ( B-NHL ) and a B-NHL cell line . The evidence of a significant proportion of loss-of-function mutations and a complete absence of the normal copy of ATM in the majority of mutated tumours establishes somatic inactivation of this gene in the pathogenesis of sporadic T-PLL and suggests that ATM acts as a tumour suppressor . As constitutional DNA was not available , a putative hereditary predisposition to T-PLL will require further investigation . . \t0\n",
      "Myotonic dystrophy protein kinase is involved in the modulation of the Ca2+ homeostasis in skeletal muscle cells. Myotonic dystrophy ( DM ) , the most prevalent muscular disorder in adults , is caused by ( CTG ) n-repeat expansion in a gene encoding a protein kinase ( DM protein kinase ; DMPK ) and involves changes in cytoarchitecture and ion homeostasis . To obtain clues to the normal biological role of DMPK in cellular ion homeostasis , we have compared the resting [ Ca2 + ] i , the amplitude and shape of depolarization-induced Ca2 + transients , and the content of ATP-driven ion pumps in cultured skeletal muscle cells of wild-type and DMPK [ - / - ] knockout mice . In vitro-differentiated DMPK [ - / - ] myotubes exhibit a higher resting [ Ca2 + ] i than do wild-type myotubes because of an altered open probability of voltage-dependent l-type Ca2 + and Na + channels . The mutant myotubes exhibit smaller and slower Ca2 + responses upon triggering by acetylcholine or high external K + . In addition , we observed that these Ca2 + transients partially result from an influx of extracellular Ca2 + through the l-type Ca2 + channel . Neither the content nor the activity of Na + / K + ATPase and sarcoplasmic reticulum Ca2 + -ATPase are affected by DMPK absence . In conclusion , our data suggest that DMPK is involved in modulating the initial events of excitation-contraction coupling in skeletal muscle . . \t1\n",
      "Constitutional RB1-gene mutations in patients with isolated unilateral retinoblastoma. In most patients with isolated unilateral retinoblastoma , tumor development is initiated by somatic inactivation of both alleles of the RB1 gene . However , some of these patients can transmit retinoblastoma predisposition to their offspring . To determine the frequency and nature of constitutional RB1-gene mutations in patients with isolated unilateral retinoblastoma , we analyzed DNA from peripheral blood and from tumor tissue . The analysis of tumors from 54 ( 71 % ) of 76 informative patients showed loss of constitutional heterozygosity ( LOH ) at intragenic loci . Three of 13 uninformative patients had constitutional deletions . For 39 randomly selected tumors , SSCP , hetero-duplex analysis , sequencing , and Southern blot analysis were used to identify mutations . Mutations were detected in 21 ( 91 % ) of 23 tumors with LOH . In 6 ( 38 % ) of 16 tumors without LOH , one mutation was detected , and in 9 ( 56 % ) of the tumors without LOH , both mutations were found . Thus , a total of 45 mutations were identified in tumors of 36 patients . Thirty-nine of the mutations-including 34 small mutations , 2 large structural alterations , and hypermethylation in 3 tumors-were not detected in the corresponding peripheral blood DNA . In 6 ( 17 % ) of the 36 patients , a mutation was detected in constitutional DNA , and 1 of these mutations is known to be associated with reduced expressivity . The presence of a constitutional mutation was not associated with an early age at treatment . In 1 patient , somatic mosaicism was demonstrated by molecular analysis of DNA and RNA from peripheral blood . In 2 patients without a detectable mutation in peripheral blood , mosaicism was suggested because 1 of the patients showed multifocal tumors and the other later developed bilateral retinoblastoma . In conclusion , our results emphasize that the manifestation and transmissibility of retinoblastoma depend on the nature of the first mutation , its time in development , and the number and types of cells that are affected . . \t2\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the tab separated data\n",
    "# \"1\" is \"positive\" and \"0\" is \"negative\"\n",
    "print(\"*****\\ntrain_nemo_format.tsv sample\\n*****\")\n",
    "!head -n 3 $TC3_DATA_DIR/train_nemo_format.tsv\n",
    "print(\"\\n\\n*****\\ndev_nemo_format.tsv sample\\n*****\")\n",
    "!head -n 3 $TC3_DATA_DIR/dev_nemo_format.tsv\n",
    "print(\"\\n\\n*****\\ntest_nemo_format.tsv sample\\n*****\")\n",
    "!head -n 3 $TC3_DATA_DIR/test_nemo_format.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dli/task/data/NCBI_tc-3/dev.tsv\n",
      "/dli/task/data/NCBI_tc-3/dev_nemo_format.tsv\n",
      "/dli/task/data/NCBI_tc-3/test.tsv\n",
      "/dli/task/data/NCBI_tc-3/test_nemo_format.tsv\n",
      "/dli/task/data/NCBI_tc-3/train.tsv\n",
      "/dli/task/data/NCBI_tc-3/train_nemo_format.tsv\n"
     ]
    }
   ],
   "source": [
    "TC3_DATA_DIR = '/dli/task/data/NCBI_tc-3'\n",
    "!ls $TC3_DATA_DIR/*.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2 Configuration File\n",
    "List the config file `text_classification_config.yaml` and take a look at the keys and default values.  Note the hierarchy of the keys and, especially, the three top-level keys: `trainer`, `model`, and `exp_manager`.\n",
    "\n",
    "```yaml\n",
    "trainer:\n",
    "  gpus:\n",
    "  num_nodes:\n",
    "  max_epochs:\n",
    "  ...\n",
    "  \n",
    "model:\n",
    "  nemo_path:\n",
    "  tokenizer:  \n",
    "  language_model:\n",
    "  classifier_head:\n",
    "  ...\n",
    "\n",
    "exp_manager:\n",
    "  ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "# Config file for text classification with pre-trained BERT models\n",
      "\n",
      "trainer:\n",
      "  gpus: 1 # number of GPUs (0 for CPU), or list of the GPUs to use e.g. [0, 1]\n",
      "  num_nodes: 1\n",
      "  max_epochs: 100\n",
      "  max_steps: null # precedence over max_epochs\n",
      "  accumulate_grad_batches: 1 # accumulates grads every k batches\n",
      "  gradient_clip_val: 0.0\n",
      "  amp_level: O0 # O1/O2 for mixed precision\n",
      "  precision: 32 # Should be set to 16 for O1 and O2 to enable the AMP.\n",
      "  accelerator: ddp\n",
      "  log_every_n_steps: 1  # Interval of logging.\n",
      "  val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations\n",
      "  resume_from_checkpoint: null # The path to a checkpoint file to continue the training, restores the whole state including the epoch, step, LR schedulers, apex, etc.\n",
      "  num_sanity_val_steps: 0 # number of steps to perform validation steps for sanity check the validation process before starting the training, setting to 0 disables it\n",
      "\n",
      "  checkpoint_callback: False  # Provided by exp_manager\n",
      "  logger: False  # Provided by exp_manager\n",
      "\n",
      "model:\n",
      "  nemo_path: text_classification_model.nemo # filename to save the model and associated artifacts to .nemo file\n",
      "  tokenizer:\n",
      "      tokenizer_name: ${model.language_model.pretrained_model_name} # or sentencepiece\n",
      "      vocab_file: null # path to vocab file\n",
      "      tokenizer_model: null # only used if tokenizer is sentencepiece\n",
      "      special_tokens: null\n",
      "\n",
      "  language_model:\n",
      "    pretrained_model_name: bert-base-uncased\n",
      "    lm_checkpoint: null\n",
      "    config_file: null # json file, precedence over config\n",
      "    config: null\n",
      "\n",
      "  classifier_head:\n",
      "    num_output_layers: 2\n",
      "    fc_dropout: 0.1\n",
      "\n",
      "  class_labels:\n",
      "    class_labels_file : null # optional to specify a file containing the list of the labels\n",
      "\n",
      "  dataset:\n",
      "    num_classes: ??? # The number of classes. 0 < Label <num_classes.\n",
      "    do_lower_case: false # true for uncased models, false for cased models, will be set automatically if pre-trained tokenizer model is used\n",
      "    max_seq_length: 256 # the maximum length BERT supports is 512\n",
      "    class_balancing: null # null or 'weighted_loss'. 'weighted_loss' enables the weighted class balancing of the loss, may be used for handling unbalanced classes\n",
      "    use_cache: false # uses a cache to store the processed dataset, you may use it for large datasets for speed up\n",
      "\n",
      "  train_ds:\n",
      "    file_path: null\n",
      "    batch_size: 64\n",
      "    shuffle: true\n",
      "    num_samples: -1 # number of samples to be considered, -1 means all the dataset\n",
      "    num_workers: 3\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "\n",
      "  validation_ds:\n",
      "    file_path: null\n",
      "    batch_size: 64\n",
      "    shuffle: false\n",
      "    num_samples: -1 # number of samples to be considered, -1 means all the dataset\n",
      "    num_workers: 3\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "\n",
      "  test_ds:\n",
      "    file_path: null\n",
      "    batch_size: 64\n",
      "    shuffle: false\n",
      "    num_samples: -1 # number of samples to be considered, -1 means all the dataset\n",
      "    num_workers: 3\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "\n",
      "  optim:\n",
      "    name: adam\n",
      "    lr: 2e-5\n",
      "    # optimizer arguments\n",
      "    betas: [0.9, 0.999]\n",
      "    weight_decay: 0.01\n",
      "\n",
      "    # scheduler setup\n",
      "    sched:\n",
      "      name: WarmupAnnealing\n",
      "      # Scheduler params\n",
      "      warmup_steps: null\n",
      "      warmup_ratio: 0.1\n",
      "      last_epoch: -1\n",
      "      # pytorch lightning args\n",
      "      monitor: val_loss\n",
      "      reduce_on_plateau: false\n",
      "\n",
      "  # List of some sample queries for inference after training is done\n",
      "  infer_samples: [\n",
      "    'by the end of no such thing the audience , like beatrice , has a watchful affection for the monster .',\n",
      "    'director rob marshall went out gunning to make a great one .',\n",
      "    'uneasy mishmash of styles and genres .',\n",
      "  ]\n",
      "\n",
      "exp_manager:\n",
      "  exp_dir: null  # exp_dir for your experiment, if None, defaults to \"./nemo_experiments\"\n",
      "  name: \"TextClassification\"  # The name of your model\n",
      "  create_tensorboard_logger: True  # Whether you want exp_manger to create a tb logger\n",
      "  create_checkpoint_callback: True  # Whether you want exp_manager to create a modelcheckpoint callback\n"
     ]
    }
   ],
   "source": [
    "CONFIG_DIR = \"/dli/task/nemo/examples/nlp/text_classification/conf\"\n",
    "CONFIG_FILE = \"text_classification_config.yaml\"\n",
    "!cat $CONFIG_DIR/$CONFIG_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2.1 OmegaConf Tool\n",
    "The YAML config file provides default values for most of the parameters, but there are a few items that must be specified for the text classification experiment in order to run it at all.  \n",
    "\n",
    "Each YAML section is a bit easier to view using the [omegaconf](https://omegaconf.readthedocs.io/en/2.1_branch/#) package, which allows you to access and manipulate the configuration keys using a \"dot\" protocol.  \n",
    "\n",
    "Start by instantiating an `OmegaConf` object from the config file. Keys in the object can be changed, added, viewed, saved, and so on.  \n",
    "\n",
    "For example, to look at just the `model` section, we can load the config file and specify just the `config.model` section to view through a print statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nemo_path: text_classification_model.nemo\n",
      "tokenizer:\n",
      "  tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "  vocab_file: null\n",
      "  tokenizer_model: null\n",
      "  special_tokens: null\n",
      "language_model:\n",
      "  pretrained_model_name: bert-base-uncased\n",
      "  lm_checkpoint: null\n",
      "  config_file: null\n",
      "  config: null\n",
      "classifier_head:\n",
      "  num_output_layers: 2\n",
      "  fc_dropout: 0.1\n",
      "class_labels:\n",
      "  class_labels_file: null\n",
      "dataset:\n",
      "  num_classes: ???\n",
      "  do_lower_case: false\n",
      "  max_seq_length: 256\n",
      "  class_balancing: null\n",
      "  use_cache: false\n",
      "train_ds:\n",
      "  file_path: null\n",
      "  batch_size: 64\n",
      "  shuffle: true\n",
      "  num_samples: -1\n",
      "  num_workers: 3\n",
      "  drop_last: false\n",
      "  pin_memory: false\n",
      "validation_ds:\n",
      "  file_path: null\n",
      "  batch_size: 64\n",
      "  shuffle: false\n",
      "  num_samples: -1\n",
      "  num_workers: 3\n",
      "  drop_last: false\n",
      "  pin_memory: false\n",
      "test_ds:\n",
      "  file_path: null\n",
      "  batch_size: 64\n",
      "  shuffle: false\n",
      "  num_samples: -1\n",
      "  num_workers: 3\n",
      "  drop_last: false\n",
      "  pin_memory: false\n",
      "optim:\n",
      "  name: adam\n",
      "  lr: 2.0e-05\n",
      "  betas:\n",
      "  - 0.9\n",
      "  - 0.999\n",
      "  weight_decay: 0.01\n",
      "  sched:\n",
      "    name: WarmupAnnealing\n",
      "    warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    monitor: val_loss\n",
      "    reduce_on_plateau: false\n",
      "infer_samples:\n",
      "- by the end of no such thing the audience , like beatrice , has a watchful affection\n",
      "  for the monster .\n",
      "- director rob marshall went out gunning to make a great one .\n",
      "- uneasy mishmash of styles and genres .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "config = OmegaConf.load(CONFIG_DIR + \"/\" + CONFIG_FILE)\n",
    "print(OmegaConf.to_yaml(config.model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details about the model arguments can be found in the [documentation](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/text_classification.html#model-arguments).  The `dataset.num_classes` value as well as locations of the data in `train_ds.file_path`, `val_ds.file_path`, and `test_ds.file_path` are required.\n",
    "\n",
    "To make sure we don't run out of memory, we can limit the `dataset.max_seq_length` to 128.  It also looks like the `infer_samples` are related to movie reviews, so we can change those to sentences that are meaningful in the disease domain.\n",
    "\n",
    "There are some other parameters we might want to change later, but for now, this is all we absolutely must provide.  \n",
    "\n",
    "Next take a look at the `trainer` subsection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpus: 1\n",
      "num_nodes: 1\n",
      "max_epochs: 100\n",
      "max_steps: null\n",
      "accumulate_grad_batches: 1\n",
      "gradient_clip_val: 0.0\n",
      "amp_level: O0\n",
      "precision: 32\n",
      "accelerator: ddp\n",
      "log_every_n_steps: 1\n",
      "val_check_interval: 1.0\n",
      "resume_from_checkpoint: null\n",
      "num_sanity_val_steps: 0\n",
      "checkpoint_callback: false\n",
      "logger: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(config.trainer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only have one GPU right now, so that setting is fine, but we might want to limit the `max_epochs` to just a few to start with.  As with the `model` configs, there are some other parameters we might want to change, but we can go with the default for our first try.  \n",
    "\n",
    "Finally, what about the `exp_manager`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_dir: null\n",
      "name: TextClassification\n",
      "create_tensorboard_logger: true\n",
      "create_checkpoint_callback: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(config.exp_manager))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is fine as it is. When the `exp_dir` is `null`, it will default to placing the experiment results in a new directory named `nemo_experiments`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.3 Hydra-Enabled Python Script\n",
    "To recap, the parameters we need to change or override are:\n",
    "\n",
    "* `model.dataset.num_classes`: set to 3\n",
    "* `model.dataset.max_seq_length`: set to 128\n",
    "* `model.train_ds.file_path`: set to train_nemo_format.tsv\n",
    "* `model.val_ds.file_path`: set to dev_nemo_format.tsv\n",
    "* `model.test_ds.file_path`: set to test_nemo_format.tsv\n",
    "* `model.infer_samples` : set to relevent sentences\n",
    "* `trainer.max_epochs`: set to 3\n",
    "\n",
    "We can train, infer, and test it all **in one command** using the text classification training script!  \n",
    "\n",
    "The script uses Hydra to manage the config file, so that means we can just override the values we want to from the command line as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-12 11:21:15 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/omegaconf/basecontainer.py:225: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "    Use OmegaConf.to_yaml(cfg)\n",
      "    \n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo I 2024-03-12 11:21:15 text_classification_with_bert:110] \n",
      "    Config Params:\n",
      "    trainer:\n",
      "      gpus: 1\n",
      "      num_nodes: 1\n",
      "      max_epochs: 3\n",
      "      max_steps: null\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 0.0\n",
      "      amp_level: O0\n",
      "      precision: 32\n",
      "      accelerator: ddp\n",
      "      log_every_n_steps: 1\n",
      "      val_check_interval: 1.0\n",
      "      resume_from_checkpoint: null\n",
      "      num_sanity_val_steps: 0\n",
      "      checkpoint_callback: false\n",
      "      logger: false\n",
      "    model:\n",
      "      nemo_path: text_classification_model.nemo\n",
      "      tokenizer:\n",
      "        tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "        vocab_file: null\n",
      "        tokenizer_model: null\n",
      "        special_tokens: null\n",
      "      language_model:\n",
      "        pretrained_model_name: bert-base-uncased\n",
      "        lm_checkpoint: null\n",
      "        config_file: null\n",
      "        config: null\n",
      "      classifier_head:\n",
      "        num_output_layers: 2\n",
      "        fc_dropout: 0.1\n",
      "      class_labels:\n",
      "        class_labels_file: null\n",
      "      dataset:\n",
      "        num_classes: 3\n",
      "        do_lower_case: false\n",
      "        max_seq_length: 128\n",
      "        class_balancing: null\n",
      "        use_cache: false\n",
      "      train_ds:\n",
      "        file_path: /dli/task/data/NCBI_tc-3/train_nemo_format.tsv\n",
      "        batch_size: 64\n",
      "        shuffle: true\n",
      "        num_samples: -1\n",
      "        num_workers: 3\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "      validation_ds:\n",
      "        file_path: /dli/task/data/NCBI_tc-3/dev_nemo_format.tsv\n",
      "        batch_size: 64\n",
      "        shuffle: false\n",
      "        num_samples: -1\n",
      "        num_workers: 3\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "      test_ds:\n",
      "        file_path: /dli/task/data/NCBI_tc-3/test_nemo_format.tsv\n",
      "        batch_size: 64\n",
      "        shuffle: false\n",
      "        num_samples: -1\n",
      "        num_workers: 3\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "      optim:\n",
      "        name: adam\n",
      "        lr: 2.0e-05\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.999\n",
      "        weight_decay: 0.01\n",
      "        sched:\n",
      "          name: WarmupAnnealing\n",
      "          warmup_steps: null\n",
      "          warmup_ratio: 0.1\n",
      "          last_epoch: -1\n",
      "          monitor: val_loss\n",
      "          reduce_on_plateau: false\n",
      "      infer_samples:\n",
      "      - In contrast no mutations were detected in the p53 gene suggesting that this tumour\n",
      "        suppressor is not frequently altered in this leukaemia\n",
      "      - The first predictive testing for Huntington disease  was based on analysis of\n",
      "        linked polymorphic DNA markers to estimate the likelihood of inheriting the mutation\n",
      "        for HD\n",
      "      - Further studies suggested that low dilutions of C5D serum contain a factor or\n",
      "        factors interfering at some step in the hemolytic assay of C5 rather than a true\n",
      "        C5 inhibitor or inactivator\n",
      "    exp_manager:\n",
      "      exp_dir: null\n",
      "      name: TextClassification\n",
      "      create_tensorboard_logger: true\n",
      "      create_checkpoint_callback: true\n",
      "    \n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "[NeMo I 2024-03-12 11:21:15 exp_manager:216] Experiments will be logged at /dli/task/nemo_experiments/TextClassification/2024-03-12_11-21-15\n",
      "[NeMo I 2024-03-12 11:21:15 exp_manager:563] TensorboardLogger has been set up\n",
      "Lock 140423431659280 acquired on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Downloading: 100%|██████████████████████████████| 570/570 [00:00<00:00, 825kB/s]\n",
      "Lock 140423431659280 released on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Lock 140423431629600 acquired on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Downloading: 100%|███████████████████████████| 232k/232k [00:00<00:00, 48.3MB/s]\n",
      "Lock 140423431629600 released on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Lock 140423432108208 acquired on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.76ea01b4b85ac16e2cec55c398cba7a943d89ab21dfdd973f6630a152e4b9aed.lock\n",
      "Downloading: 100%|███████████████████████████| 48.0/48.0 [00:00<00:00, 59.7kB/s]\n",
      "Lock 140423432108208 released on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.76ea01b4b85ac16e2cec55c398cba7a943d89ab21dfdd973f6630a152e4b9aed.lock\n",
      "Lock 140423431629600 acquired on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Downloading: 100%|███████████████████████████| 466k/466k [00:00<00:00, 44.9MB/s]\n",
      "Lock 140423431629600 released on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo I 2024-03-12 11:21:16 text_classification_dataset:120] Read 683 examples from /dli/task/data/NCBI_tc-3/train_nemo_format.tsv.\n",
      "[NeMo I 2024-03-12 11:21:16 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2024-03-12 11:21:16 text_classification_dataset:239] example 0: ['Genetic', 'basis', 'and', 'molecular', 'mechanism', 'for', 'idiopathic', 'ventricular', 'fibrillation.', 'Ventricular', 'fibrillation', 'causes', 'more', 'than', '300', ',', '000', 'sudden', 'deaths', 'each', 'year', 'in', 'the', 'USA', 'alone', '.', 'In', 'approximately', '5-12', '%', 'of', 'these', 'cases', ',', 'there', 'are', 'no', 'demonstrable', 'cardiac', 'or', 'non-cardiac', 'causes', 'to', 'account', 'for', 'the', 'episode', ',', 'which', 'is', 'therefore', 'classified', 'as', 'idiopathic', 'ventricular', 'fibrillation', '(', 'IVF', ')', '.', 'A', 'distinct', 'group', 'of', 'IVF', 'patients', 'has', 'been', 'found', 'to', 'present', 'with', 'a', 'characteristic', 'electrocardiographic', 'pattern', '.', 'Because', 'of', 'the', 'small', 'size', 'of', 'most', 'pedigrees', 'and', 'the', 'high', 'incidence', 'of', 'sudden', 'death', ',', 'however', ',', 'molecular', 'genetic', 'studies', 'of', 'IVF', 'have', 'not', 'yet', 'been', 'done', '.', 'Because', 'IVF', 'causes', 'cardiac', 'rhythm', 'disturbance', ',', 'we', 'investigated', 'whether', 'malfunction', 'of', 'ion', 'channels', 'could', 'cause', 'the', 'disorder', 'by', 'studying', 'mutations', 'in', 'the', 'cardiac', 'sodium', 'channel', 'gene', 'SCN5A', '.', 'We', 'have', 'now', 'identified', 'a', 'missense', 'mutation', ',', 'a', 'splice-donor', 'mutation', ',', 'and', 'a', 'frameshift', 'mutation', 'in', 'the', 'coding', 'region', 'of', 'SCN5A', 'in', 'three', 'IVF', 'families', '.', 'We', 'show', 'that', 'sodium', 'channels', 'with', 'the', 'missense', 'mutation', 'recover', 'from', 'inactivation', 'more', 'rapidly', 'than', 'normal', 'and', 'that', 'the', 'frameshift', 'mutation', 'causes', 'the', 'sodium', 'channel', 'to', 'be', 'non-functional', '.', 'Our', 'results', 'indicate', 'that', 'mutations', 'in', 'cardiac', 'ion-channel', 'genes', 'contribute', 'to', 'the', 'risk', 'of', 'developing', 'IVF', '.', '.']\n",
      "[NeMo I 2024-03-12 11:21:16 text_classification_dataset:240] subtokens: [CLS] genetic basis and molecular mechanism for id ##io ##pathic vent ##ric ##ular fi ##bri ##llation . vent ##ric ##ular fi ##bri ##llation causes more than 300 , 000 sudden deaths each year in the usa alone . in approximately 5 - 12 % of these cases , there are no demons ##tra ##ble cardiac or non - cardiac causes to account for the episode , which is therefore classified as id ##io ##pathic vent ##ric ##ular fi ##bri ##llation ( iv ##f ) . a distinct group of iv ##f patients has been found to present with a characteristic electro ##card ##io ##graphic pattern . because of the small size of most pe ##di ##gree ##s and the high incidence of sudden death , however , molecular [SEP]\n",
      "[NeMo I 2024-03-12 11:21:16 text_classification_dataset:241] input_ids: 101 7403 3978 1998 8382 7337 2005 8909 3695 25940 18834 7277 7934 10882 23736 20382 1012 18834 7277 7934 10882 23736 20382 5320 2062 2084 3998 1010 2199 5573 6677 2169 2095 1999 1996 3915 2894 1012 1999 3155 1019 1011 2260 1003 1997 2122 3572 1010 2045 2024 2053 7942 6494 3468 15050 2030 2512 1011 15050 5320 2000 4070 2005 1996 2792 1010 2029 2003 3568 6219 2004 8909 3695 25940 18834 7277 7934 10882 23736 20382 1006 4921 2546 1007 1012 1037 5664 2177 1997 4921 2546 5022 2038 2042 2179 2000 2556 2007 1037 8281 16175 11522 3695 14773 5418 1012 2138 1997 1996 2235 2946 1997 2087 21877 4305 28637 2015 1998 1996 2152 18949 1997 5573 2331 1010 2174 1010 8382 102\n",
      "[NeMo I 2024-03-12 11:21:16 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-03-12 11:21:16 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2024-03-12 11:21:16 text_classification_dataset:244] label: 2\n",
      "[NeMo I 2024-03-12 11:21:16 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2024-03-12 11:21:16 text_classification_dataset:239] example 1: ['High', 'residual', 'arylsulfatase', 'A', '(ARSA)', 'activity', 'in', 'a', 'patient', 'with', 'late-infantile', 'metachromatic', 'leukodystrophy.', 'We', 'identified', 'a', 'patient', 'suffering', 'from', 'late-infantile', 'metachromatic', 'leukodystrophy', '(', 'MLD', ')', 'who', 'has', 'a', 'residual', 'arylsulfatase', 'A', '(', 'ARSA', ')', 'activity', 'of', 'about', '10', '%', '.', 'Fibroblasts', 'of', 'the', 'patient', 'show', 'significant', 'sulfatide', 'degradation', 'activity', 'exceeding', 'that', 'of', 'adult', 'MLD', 'patients', '.', 'Analysis', 'of', 'the', 'ARSA', 'gene', 'in', 'this', 'patient', 'revealed', 'heterozygosity', 'for', 'two', 'new', 'mutant', 'alleles', 'in', 'one', 'allele', ',', 'deletion', 'of', 'C', '447', 'in', 'exon', '2', 'leads', 'to', 'a', 'frameshift', 'and', 'to', 'a', 'premature', 'stop', 'codon', 'at', 'amino', 'acid', 'position', '105', ';', 'in', 'the', 'second', 'allele', ',', 'a', 'G--', '>', 'A', 'transition', 'in', 'exon', '5', 'causes', 'a', 'Gly309--', '>', 'Ser', 'substitution', '.', 'Transient', 'expression', 'of', 'the', 'mutant', 'Ser309-ARSA', 'resulted', 'in', 'only', '13', '%', 'enzyme', 'activity', 'of', 'that', 'observed', 'in', 'cells', 'expressing', 'normal', 'ARSA', '.', 'The', 'mutant', 'ARSA', 'is', 'correctly', 'targeted', 'to', 'the', 'lysosomes', 'but', 'is', 'unstable', '.', 'These', 'findings', 'are', 'in', 'contrast', 'to', 'previous', 'results', 'showing', 'that', 'the', 'late-infantile', 'type', 'of', 'MLD', 'is', 'always', 'associated', 'with', 'the', 'complete', 'absence', 'of', 'ARSA', 'activity', '.', 'The', 'expression', 'of', 'the', 'mutant', 'ARSA', 'protein', 'may', 'be', 'influenced', 'by', 'particular', 'features', 'of', 'oligodendrocytes', ',', 'such', 'that', 'the', 'level', 'of', 'mutant', 'enzyme', 'is', 'lower', 'in', 'these', 'cells', 'than', 'in', 'others', '.', '.']\n",
      "[NeMo I 2024-03-12 11:21:16 text_classification_dataset:240] subtokens: [CLS] high residual ar ##yl ##sul ##fat ##ase a ( ars ##a ) activity in a patient with late - infant ##ile meta ##ch ##romatic le ##uk ##od ##yst ##rop ##hy . we identified a patient suffering from late - infant ##ile meta ##ch ##romatic le ##uk ##od ##yst ##rop ##hy ( ml ##d ) who has a residual ar ##yl ##sul ##fat ##ase a ( ars ##a ) activity of about 10 % . fi ##bro ##bla ##sts of the patient show significant sul ##fat ##ide degradation activity exceeding that of adult ml ##d patients . analysis of the ars ##a gene in this patient revealed het ##ero ##zy ##gos ##ity for two new mutant all ##eles in one all ##ele , del ##eti ##on of c [SEP]\n",
      "[NeMo I 2024-03-12 11:21:16 text_classification_dataset:241] input_ids: 101 2152 21961 12098 8516 23722 27753 11022 1037 1006 29393 2050 1007 4023 1999 1037 5776 2007 2397 1011 10527 9463 18804 2818 23645 3393 6968 7716 27268 18981 10536 1012 2057 4453 1037 5776 6114 2013 2397 1011 10527 9463 18804 2818 23645 3393 6968 7716 27268 18981 10536 1006 19875 2094 1007 2040 2038 1037 21961 12098 8516 23722 27753 11022 1037 1006 29393 2050 1007 4023 1997 2055 2184 1003 1012 10882 12618 28522 12837 1997 1996 5776 2265 3278 21396 27753 5178 16627 4023 17003 2008 1997 4639 19875 2094 5022 1012 4106 1997 1996 29393 2050 4962 1999 2023 5776 3936 21770 10624 9096 12333 3012 2005 2048 2047 15527 2035 26741 1999 2028 2035 12260 1010 3972 20624 2239 1997 1039 102\n",
      "[NeMo I 2024-03-12 11:21:16 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-03-12 11:21:16 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2024-03-12 11:21:16 text_classification_dataset:244] label: 1\n",
      "[NeMo W 2024-03-12 11:21:26 text_classification_dataset:250] Found 664 out of 683 sentences with more than 128 subtokens. Truncated long sentences from the end.\n",
      "[NeMo I 2024-03-12 11:21:26 data_preprocessing:299] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2024-03-12 11:21:26 data_preprocessing:301] Min: 74 |                  Max: 129 |                  Mean: 128.36163982430455 |                  Median: 129.0\n",
      "[NeMo I 2024-03-12 11:21:26 data_preprocessing:307] 75 percentile: 129.00\n",
      "[NeMo I 2024-03-12 11:21:26 data_preprocessing:308] 99 percentile: 129.00\n",
      "[NeMo I 2024-03-12 11:21:26 text_classification_dataset:120] Read 100 examples from /dli/task/data/NCBI_tc-3/dev_nemo_format.tsv.\n",
      "[NeMo I 2024-03-12 11:21:26 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2024-03-12 11:21:26 text_classification_dataset:239] example 0: ['BRCA1', 'is', 'secreted', 'and', 'exhibits', 'properties', 'of', 'a', 'granin.', 'Germline', 'mutations', 'in', 'BRCA1', 'are', 'responsible', 'for', 'most', 'cases', 'of', 'inherited', 'breast', 'and', 'ovarian', 'cancer', '.', 'However', ',', 'the', 'function', 'of', 'the', 'BRCA1', 'protein', 'has', 'remained', 'elusive', '.', 'We', 'now', 'show', 'that', 'BRCA1', 'encodes', 'a', '190-kD', 'protein', 'with', 'sequence', 'homology', 'and', 'biochemical', 'analogy', 'to', 'the', 'granin', 'protein', 'family', '.', 'Interestingly', ',', 'BRCA2', 'also', 'includes', 'a', 'motif', 'similar', 'to', 'the', 'granin', 'consensus', 'at', 'the', 'C', 'terminus', 'of', 'the', 'protein', '.', 'Both', 'BRCA1', 'and', 'the', 'granins', 'localize', 'to', 'secretory', 'vesicles', ',', 'are', 'secreted', 'by', 'a', 'regulated', 'pathway', ',', 'are', 'post-translationally', 'glycosylated', 'and', 'are', 'responsive', 'to', 'hormones', '.', 'As', 'a', 'regulated', 'secretory', 'protein', ',', 'BRCA1', 'appears', 'to', 'function', 'by', 'a', 'mechanism', 'not', 'previously', 'described', 'for', 'tumour', 'suppressor', 'gene', 'products', '.', '.']\n",
      "[NeMo I 2024-03-12 11:21:26 text_classification_dataset:240] subtokens: [CLS] br ##ca ##1 is secret ##ed and exhibits properties of a gran ##in . ge ##rm ##line mutations in br ##ca ##1 are responsible for most cases of inherited breast and o ##var ##ian cancer . however , the function of the br ##ca ##1 protein has remained elusive . we now show that br ##ca ##1 en ##codes a 190 - k ##d protein with sequence homo ##logy and bio ##chemical analogy to the gran ##in protein family . interesting ##ly , br ##ca ##2 also includes a motif similar to the gran ##in consensus at the c terminus of the protein . both br ##ca ##1 and the gran ##ins local ##ize to secret ##ory ve ##sic ##les , are secret ##ed by a regulated [SEP]\n",
      "[NeMo I 2024-03-12 11:21:26 text_classification_dataset:241] input_ids: 101 7987 3540 2487 2003 3595 2098 1998 10637 5144 1997 1037 12604 2378 1012 16216 10867 4179 14494 1999 7987 3540 2487 2024 3625 2005 2087 3572 1997 7900 7388 1998 1051 10755 2937 4456 1012 2174 1010 1996 3853 1997 1996 7987 3540 2487 5250 2038 2815 26475 1012 2057 2085 2265 2008 7987 3540 2487 4372 23237 1037 11827 1011 1047 2094 5250 2007 5537 24004 6483 1998 16012 15869 23323 2000 1996 12604 2378 5250 2155 1012 5875 2135 1010 7987 3540 2475 2036 2950 1037 16226 2714 2000 1996 12604 2378 10465 2012 1996 1039 7342 1997 1996 5250 1012 2119 7987 3540 2487 1998 1996 12604 7076 2334 4697 2000 3595 10253 2310 19570 4244 1010 2024 3595 2098 2011 1037 12222 102\n",
      "[NeMo I 2024-03-12 11:21:26 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-03-12 11:21:26 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2024-03-12 11:21:26 text_classification_dataset:244] label: 0\n",
      "[NeMo I 2024-03-12 11:21:26 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2024-03-12 11:21:26 text_classification_dataset:239] example 1: ['Ovarian', 'cancer', 'risk', 'in', 'BRCA1', 'carriers', 'is', 'modified', 'by', 'the', 'HRAS1', 'variable', 'number', 'of', 'tandem', 'repeat', '(VNTR)', 'locus.', 'Women', 'who', 'carry', 'a', 'mutation', 'in', 'the', 'BRCA1', 'gene', '(', 'on', 'chromosome', '17q21', ')', ',', 'have', 'an', '80', '%', 'risk', 'of', 'breast', 'cancer', 'and', 'a', '40', '%', 'risk', 'of', 'ovarian', 'cancer', 'by', 'the', 'age', 'of', '70', '(', 'ref', '.', '1', ')', '.', 'The', 'variable', 'penetrance', 'of', 'BRCA1', 'suggests', 'that', 'other', 'genetic', 'and', 'non-genetic', 'factors', 'play', 'a', 'role', 'in', 'tumourigenesis', 'in', 'these', 'individuals', '.', 'The', 'HRAS1', 'variable', 'number', 'of', 'tandem', 'repeats', '(', 'VNTR', ')', 'polymorphism', ',', 'located', '1', 'kilobase', '(', 'kb', ')', 'downstream', 'of', 'the', 'HRAS1', 'proto-oncogene', '(', 'chromosome', '11p15', '.', '5', ')', 'is', 'one', 'possible', 'genetic', 'modifier', 'of', 'cancer', 'penetrance', '.', 'Individuals', 'who', 'have', 'rare', 'alleles', 'of', 'the', 'VNTR', 'have', 'an', 'increased', 'risk', 'of', 'certain', 'types', 'of', 'cancers', ',', 'including', 'breast', 'cancer', '(', '2-4', ')', '.', 'To', 'investigate', 'whether', 'the', 'presence', 'of', 'rare', 'HRAS1', 'alleles', 'increases', 'susceptibility', 'to', 'hereditary', 'breast', 'and', 'ovarian', 'cancer', ',', 'we', 'have', 'typed', 'a', 'panel', 'of', '307', 'female', 'BRCA1', 'carriers', 'at', 'this', 'locus', 'using', 'a', 'PCR-based', 'technique', '.', 'The', 'risk', 'for', 'ovarian', 'cancer', 'was', '2', '.', '11', 'times', 'greater', 'for', 'BRCA1', 'carriers', 'harbouring', 'one', 'or', 'two', 'rare', 'HRAS1', 'alleles', ',', 'compared', 'to', 'carriers', 'with', 'only', 'common', 'alleles', '(', 'P', '=', '0', '.', '015', ')', '.', 'The', 'magnitude', 'of', 'the', 'relative', 'risk', 'associated', 'with', 'a', 'rare', 'HRAS1', 'allele', 'was', 'not', 'altered', 'by', 'adjusting', 'for', 'the', 'other', 'known', 'risk', 'factors', 'for', 'hereditary', 'ovarian', 'cancer', '(', '5', ')', '.', 'Susceptibility', 'to', 'breast', 'cancer', 'did', 'not', 'appear', 'to', 'be', 'affected', 'by', 'the', 'presence', 'of', 'rare', 'HRAS1', 'alleles', '.', 'This', 'study', 'is', 'the', 'first', 'to', 'show', 'the', 'effect', 'of', 'a', 'modifying', 'gene', 'on', 'the', 'penetrance', 'of', 'an', 'inherited', 'cancer', 'syndrome']\n",
      "[NeMo I 2024-03-12 11:21:26 text_classification_dataset:240] subtokens: [CLS] o ##var ##ian cancer risk in br ##ca ##1 carriers is modified by the hr ##as ##1 variable number of tandem repeat ( v ##nt ##r ) locus . women who carry a mutation in the br ##ca ##1 gene ( on chromosome 17 ##q ##21 ) , have an 80 % risk of breast cancer and a 40 % risk of o ##var ##ian cancer by the age of 70 ( ref . 1 ) . the variable pen ##et ##rance of br ##ca ##1 suggests that other genetic and non - genetic factors play a role in tu ##mour ##igen ##esis in these individuals . the hr ##as ##1 variable number of tandem repeats ( v ##nt ##r ) poly ##morphism , located 1 ki [SEP]\n",
      "[NeMo I 2024-03-12 11:21:26 text_classification_dataset:241] input_ids: 101 1051 10755 2937 4456 3891 1999 7987 3540 2487 11363 2003 6310 2011 1996 17850 3022 2487 8023 2193 1997 18231 9377 1006 1058 3372 2099 1007 25206 1012 2308 2040 4287 1037 16221 1999 1996 7987 3540 2487 4962 1006 2006 16706 2459 4160 17465 1007 1010 2031 2019 3770 1003 3891 1997 7388 4456 1998 1037 2871 1003 3891 1997 1051 10755 2937 4456 2011 1996 2287 1997 3963 1006 25416 1012 1015 1007 1012 1996 8023 7279 3388 21621 1997 7987 3540 2487 6083 2008 2060 7403 1998 2512 1011 7403 5876 2377 1037 2535 1999 10722 20360 29206 19009 1999 2122 3633 1012 1996 17850 3022 2487 8023 2193 1997 18231 17993 1006 1058 3372 2099 1007 26572 19539 1010 2284 1015 11382 102\n",
      "[NeMo I 2024-03-12 11:21:26 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-03-12 11:21:26 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2024-03-12 11:21:26 text_classification_dataset:244] label: 0\n",
      "[NeMo W 2024-03-12 11:21:27 text_classification_dataset:250] Found 99 out of 100 sentences with more than 128 subtokens. Truncated long sentences from the end.\n",
      "[NeMo I 2024-03-12 11:21:27 data_preprocessing:299] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2024-03-12 11:21:27 data_preprocessing:301] Min: 120 |                  Max: 129 |                  Mean: 128.91 |                  Median: 129.0\n",
      "[NeMo I 2024-03-12 11:21:27 data_preprocessing:307] 75 percentile: 129.00\n",
      "[NeMo I 2024-03-12 11:21:27 data_preprocessing:308] 99 percentile: 129.00\n",
      "[NeMo I 2024-03-12 11:21:28 text_classification_dataset:120] Read 10 examples from /dli/task/data/NCBI_tc-3/test_nemo_format.tsv.\n",
      "[NeMo I 2024-03-12 11:21:28 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2024-03-12 11:21:28 text_classification_dataset:239] example 0: ['Clustering', 'of', 'missense', 'mutations', 'in', 'the', 'ataxia-telangiectasia', 'gene', 'in', 'a', 'sporadic', 'T-cell', 'leukaemia.', 'Ataxia-telangiectasia', '(', 'A-T', ')', 'is', 'a', 'recessive', 'multi-system', 'disorder', 'caused', 'by', 'mutations', 'in', 'the', 'ATM', 'gene', 'at', '11q22-q23', '(', 'ref', '.', '3', ')', '.', 'The', 'risk', 'of', 'cancer', ',', 'especially', 'lymphoid', 'neoplasias', ',', 'is', 'substantially', 'elevated', 'in', 'A-T', 'patients', 'and', 'has', 'long', 'been', 'associated', 'with', 'chromosomal', 'instability', '.', 'By', 'analysing', 'tumour', 'DNA', 'from', 'patients', 'with', 'sporadic', 'T-cell', 'prolymphocytic', 'leukaemia', '(', 'T-PLL', ')', ',', 'a', 'rare', 'clonal', 'malignancy', 'with', 'similarities', 'to', 'a', 'mature', 'T-cell', 'leukaemia', 'seen', 'in', 'A-T', ',', 'we', 'demonstrate', 'a', 'high', 'frequency', 'of', 'ATM', 'mutations', 'in', 'T-PLL', '.', 'In', 'marked', 'contrast', 'to', 'the', 'ATM', 'mutation', 'pattern', 'in', 'A-T', ',', 'the', 'most', 'frequent', 'nucleotide', 'changes', 'in', 'this', 'leukaemia', 'were', 'missense', 'mutations', '.', 'These', 'clustered', 'in', 'the', 'region', 'corresponding', 'to', 'the', 'kinase', 'domain', ',', 'which', 'is', 'highly', 'conserved', 'in', 'ATM-related', 'proteins', 'in', 'mouse', ',', 'yeast', 'and', 'Drosophila', '.', 'The', 'resulting', 'amino-acid', 'substitutions', 'are', 'predicted', 'to', 'interfere', 'with', 'ATP', 'binding', 'or', 'substrate', 'recognition', '.', 'Two', 'of', 'seventeen', 'mutated', 'T-PLL', 'samples', 'had', 'a', 'previously', 'reported', 'A-T', 'allele', '.', 'In', 'contrast', ',', 'no', 'mutations', 'were', 'detected', 'in', 'the', 'p53', 'gene', ',', 'suggesting', 'that', 'this', 'tumour', 'suppressor', 'is', 'not', 'frequently', 'altered', 'in', 'this', 'leukaemia', '.', 'Occasional', 'missense', 'mutations', 'in', 'ATM', 'were', 'also', 'found', 'in', 'tumour', 'DNA', 'from', 'patients', 'with', 'B-cell', 'non-Hodgkins', 'lymphomas', '(', 'B-NHL', ')', 'and', 'a', 'B-NHL', 'cell', 'line', '.', 'The', 'evidence', 'of', 'a', 'significant', 'proportion', 'of', 'loss-of-function', 'mutations', 'and', 'a', 'complete', 'absence', 'of', 'the', 'normal', 'copy', 'of', 'ATM', 'in', 'the', 'majority', 'of', 'mutated', 'tumours', 'establishes', 'somatic', 'inactivation', 'of', 'this', 'gene', 'in', 'the', 'pathogenesis', 'of', 'sporadic', 'T-PLL', 'and', 'suggests', 'that', 'ATM', 'acts', 'as', 'a', 'tumour', 'suppressor', '.', 'As', 'constitutional', 'DNA', 'was', 'not', 'available', ',', 'a', 'putative', 'hereditary', 'predisposition', 'to', 'T-PLL', 'will', 'require', 'further', 'investigation', '.', '.']\n",
      "[NeMo I 2024-03-12 11:21:28 text_classification_dataset:240] subtokens: [CLS] cluster ##ing of miss ##ense mutations in the ata ##xia - tel ##ang ##iec ##tas ##ia gene in a sporadic t - cell le ##uka ##emia . ata ##xia - tel ##ang ##iec ##tas ##ia ( a - t ) is a recess ##ive multi - system disorder caused by mutations in the atm gene at 11 ##q ##22 - q ##23 ( ref . 3 ) . the risk of cancer , especially l ##ym ##ph ##oid neo ##pl ##asia ##s , is substantially elevated in a - t patients and has long been associated with ch ##rom ##osomal instability . by anal ##ys ##ing tu ##mour dna from patients with sporadic t - cell pro ##ly ##mp ##ho ##cy ##tic le ##uka ##emia ( t [SEP]\n",
      "[NeMo I 2024-03-12 11:21:28 text_classification_dataset:241] input_ids: 101 9324 2075 1997 3335 16700 14494 1999 1996 29533 14787 1011 10093 5654 23783 10230 2401 4962 1999 1037 24590 1056 1011 3526 3393 15750 17577 1012 29533 14787 1011 10093 5654 23783 10230 2401 1006 1037 1011 1056 1007 2003 1037 28290 3512 4800 1011 2291 8761 3303 2011 14494 1999 1996 27218 4962 2012 2340 4160 19317 1011 1053 21926 1006 25416 1012 1017 1007 1012 1996 3891 1997 4456 1010 2926 1048 24335 8458 9314 9253 24759 15396 2015 1010 2003 12381 8319 1999 1037 1011 1056 5022 1998 2038 2146 2042 3378 2007 10381 21716 27642 18549 1012 2011 20302 7274 2075 10722 20360 6064 2013 5022 2007 24590 1056 1011 3526 4013 2135 8737 6806 5666 4588 3393 15750 17577 1006 1056 102\n",
      "[NeMo I 2024-03-12 11:21:28 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-03-12 11:21:28 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2024-03-12 11:21:28 text_classification_dataset:244] label: 0\n",
      "[NeMo I 2024-03-12 11:21:28 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2024-03-12 11:21:28 text_classification_dataset:239] example 1: ['Myotonic', 'dystrophy', 'protein', 'kinase', 'is', 'involved', 'in', 'the', 'modulation', 'of', 'the', 'Ca2+', 'homeostasis', 'in', 'skeletal', 'muscle', 'cells.', 'Myotonic', 'dystrophy', '(', 'DM', ')', ',', 'the', 'most', 'prevalent', 'muscular', 'disorder', 'in', 'adults', ',', 'is', 'caused', 'by', '(', 'CTG', ')', 'n-repeat', 'expansion', 'in', 'a', 'gene', 'encoding', 'a', 'protein', 'kinase', '(', 'DM', 'protein', 'kinase', ';', 'DMPK', ')', 'and', 'involves', 'changes', 'in', 'cytoarchitecture', 'and', 'ion', 'homeostasis', '.', 'To', 'obtain', 'clues', 'to', 'the', 'normal', 'biological', 'role', 'of', 'DMPK', 'in', 'cellular', 'ion', 'homeostasis', ',', 'we', 'have', 'compared', 'the', 'resting', '[', 'Ca2', '+', ']', 'i', ',', 'the', 'amplitude', 'and', 'shape', 'of', 'depolarization-induced', 'Ca2', '+', 'transients', ',', 'and', 'the', 'content', 'of', 'ATP-driven', 'ion', 'pumps', 'in', 'cultured', 'skeletal', 'muscle', 'cells', 'of', 'wild-type', 'and', 'DMPK', '[', '-', '/', '-', ']', 'knockout', 'mice', '.', 'In', 'vitro-differentiated', 'DMPK', '[', '-', '/', '-', ']', 'myotubes', 'exhibit', 'a', 'higher', 'resting', '[', 'Ca2', '+', ']', 'i', 'than', 'do', 'wild-type', 'myotubes', 'because', 'of', 'an', 'altered', 'open', 'probability', 'of', 'voltage-dependent', 'l-type', 'Ca2', '+', 'and', 'Na', '+', 'channels', '.', 'The', 'mutant', 'myotubes', 'exhibit', 'smaller', 'and', 'slower', 'Ca2', '+', 'responses', 'upon', 'triggering', 'by', 'acetylcholine', 'or', 'high', 'external', 'K', '+', '.', 'In', 'addition', ',', 'we', 'observed', 'that', 'these', 'Ca2', '+', 'transients', 'partially', 'result', 'from', 'an', 'influx', 'of', 'extracellular', 'Ca2', '+', 'through', 'the', 'l-type', 'Ca2', '+', 'channel', '.', 'Neither', 'the', 'content', 'nor', 'the', 'activity', 'of', 'Na', '+', '/', 'K', '+', 'ATPase', 'and', 'sarcoplasmic', 'reticulum', 'Ca2', '+', '-ATPase', 'are', 'affected', 'by', 'DMPK', 'absence', '.', 'In', 'conclusion', ',', 'our', 'data', 'suggest', 'that', 'DMPK', 'is', 'involved', 'in', 'modulating', 'the', 'initial', 'events', 'of', 'excitation-contraction', 'coupling', 'in', 'skeletal', 'muscle', '.', '.']\n",
      "[NeMo I 2024-03-12 11:21:28 text_classification_dataset:240] subtokens: [CLS] my ##oton ##ic d ##yst ##rop ##hy protein kinase is involved in the modulation of the ca ##2 + home ##osta ##sis in skeletal muscle cells . my ##oton ##ic d ##yst ##rop ##hy ( d ##m ) , the most prevalent muscular disorder in adults , is caused by ( ct ##g ) n - repeat expansion in a gene encoding a protein kinase ( d ##m protein kinase ; d ##mp ##k ) and involves changes in cy ##to ##ar ##chi ##tec ##ture and ion home ##osta ##sis . to obtain clues to the normal biological role of d ##mp ##k in cellular ion home ##osta ##sis , we have compared the resting [ ca ##2 + ] i , the amplitude and shape of [SEP]\n",
      "[NeMo I 2024-03-12 11:21:28 text_classification_dataset:241] input_ids: 101 2026 25862 2594 1040 27268 18981 10536 5250 21903 2003 2920 1999 1996 25502 1997 1996 6187 2475 1009 2188 28696 6190 1999 20415 6740 4442 1012 2026 25862 2594 1040 27268 18981 10536 1006 1040 2213 1007 1010 1996 2087 15157 13472 8761 1999 6001 1010 2003 3303 2011 1006 14931 2290 1007 1050 1011 9377 4935 1999 1037 4962 17181 1037 5250 21903 1006 1040 2213 5250 21903 1025 1040 8737 2243 1007 1998 7336 3431 1999 22330 3406 2906 5428 26557 11244 1998 10163 2188 28696 6190 1012 2000 6855 15774 2000 1996 3671 6897 2535 1997 1040 8737 2243 1999 12562 10163 2188 28696 6190 1010 2057 2031 4102 1996 8345 1031 6187 2475 1009 1033 1045 1010 1996 22261 1998 4338 1997 102\n",
      "[NeMo I 2024-03-12 11:21:28 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-03-12 11:21:28 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2024-03-12 11:21:28 text_classification_dataset:244] label: 1\n",
      "[NeMo W 2024-03-12 11:21:28 text_classification_dataset:250] Found 10 out of 10 sentences with more than 128 subtokens. Truncated long sentences from the end.\n",
      "[NeMo I 2024-03-12 11:21:28 data_preprocessing:299] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2024-03-12 11:21:28 data_preprocessing:301] Min: 129 |                  Max: 129 |                  Mean: 129.0 |                  Median: 129.0\n",
      "[NeMo I 2024-03-12 11:21:28 data_preprocessing:307] 75 percentile: 129.00\n",
      "[NeMo I 2024-03-12 11:21:28 data_preprocessing:308] 99 percentile: 129.00\n",
      "[NeMo W 2024-03-12 11:21:28 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact forit has already been registered.\n",
      "[NeMo W 2024-03-12 11:21:28 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:243: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      self.cfg.update_node(config_path, return_path)\n",
      "    \n",
      "Lock 140423432449040 acquired on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "Downloading: 100%|███████████████████████████| 440M/440M [00:04<00:00, 89.6MB/s]\n",
      "Lock 140423432449040 released on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertEncoder: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[NeMo I 2024-03-12 11:21:34 text_classification_with_bert:118] ===========================================================================================\n",
      "[NeMo I 2024-03-12 11:21:34 text_classification_with_bert:119] Starting training...\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo I 2024-03-12 11:21:34 modelPT:748] Optimizer config = Adam (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: [0.9, 0.999]\n",
      "        eps: 1e-08\n",
      "        lr: 2e-05\n",
      "        weight_decay: 0.01\n",
      "    )\n",
      "[NeMo I 2024-03-12 11:21:34 lr_scheduler:617] Scheduler \"<nemo.core.optim.lr_scheduler.WarmupAnnealing object at 0x7fb6e0cf2a90>\" \n",
      "    will be used during training (effective maximum steps = 33) - \n",
      "    Parameters : \n",
      "    (warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    max_steps: 33\n",
      "    )\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "\n",
      "  | Name                  | Type                 | Params\n",
      "---------------------------------------------------------------\n",
      "0 | loss                  | CrossEntropyLoss     | 0     \n",
      "1 | bert_model            | BertEncoder          | 109 M \n",
      "2 | classifier            | SequenceClassifier   | 592 K \n",
      "3 | classification_report | ClassificationReport | 0     \n",
      "---------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "440.301   Total estimated model params size (MB)\n",
      "Epoch 0:  85%|▊| 11/13 [00:03<00:00,  2.85it/s, loss=1.04, v_num=1-15, lr=1.6e-5\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 13/13 [00:04<00:00,  3.21it/s, loss=1.04, v_num=1-15, lr=1.6e-5\u001b[A[NeMo I 2024-03-12 11:21:41 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                              0.00       0.00       0.00         32\n",
      "    label_id: 1                                              0.00       0.00       0.00         24\n",
      "    label_id: 2                                             44.00     100.00      61.11         44\n",
      "    -------------------\n",
      "    micro avg                                               44.00      44.00      44.00        100\n",
      "    macro avg                                               14.67      33.33      20.37        100\n",
      "    weighted avg                                            19.36      44.00      26.89        100\n",
      "    \n",
      "Epoch 0: 100%|█| 13/13 [00:04<00:00,  3.14it/s, loss=1.04, v_num=1-15, lr=1.53e-\n",
      "                                                                                \u001b[AEpoch 0, global step 10: val_loss reached 1.10476 (best 1.10476), saving model to \"/dli/task/nemo_experiments/TextClassification/2024-03-12_11-21-15/checkpoints/TextClassification--val_loss=1.10-epoch=0.ckpt\" as top 3\n",
      "Epoch 1:  92%|▉| 12/13 [00:03<00:00,  3.62it/s, loss=1, v_num=1-15, lr=8.67e-6, \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|████████████████▌                | 1/2 [00:00<00:00,  5.05it/s]\u001b[A[NeMo I 2024-03-12 11:21:48 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                              0.00       0.00       0.00         32\n",
      "    label_id: 1                                              0.00       0.00       0.00         24\n",
      "    label_id: 2                                             44.00     100.00      61.11         44\n",
      "    -------------------\n",
      "    micro avg                                               44.00      44.00      44.00        100\n",
      "    macro avg                                               14.67      33.33      20.37        100\n",
      "    weighted avg                                            19.36      44.00      26.89        100\n",
      "    \n",
      "Epoch 1: 100%|█| 13/13 [00:03<00:00,  3.60it/s, loss=1, v_num=1-15, lr=8e-6, val\n",
      "                                                                                \u001b[AEpoch 1, global step 21: val_loss reached 1.08867 (best 1.08867), saving model to \"/dli/task/nemo_experiments/TextClassification/2024-03-12_11-21-15/checkpoints/TextClassification--val_loss=1.09-epoch=1.ckpt\" as top 3\n",
      "Epoch 2:  92%|▉| 12/13 [00:03<00:00,  3.62it/s, loss=0.969, v_num=1-15, lr=1.33e\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|████████████████▌                | 1/2 [00:00<00:00,  5.36it/s]\u001b[A[NeMo I 2024-03-12 11:21:56 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                              0.00       0.00       0.00         32\n",
      "    label_id: 1                                              0.00       0.00       0.00         24\n",
      "    label_id: 2                                             44.00     100.00      61.11         44\n",
      "    -------------------\n",
      "    micro avg                                               44.00      44.00      44.00        100\n",
      "    macro avg                                               14.67      33.33      20.37        100\n",
      "    weighted avg                                            19.36      44.00      26.89        100\n",
      "    \n",
      "Epoch 2: 100%|█| 13/13 [00:03<00:00,  3.61it/s, loss=0.969, v_num=1-15, lr=6.67e\n",
      "                                                                                \u001b[AEpoch 2, global step 32: val_loss reached 1.06901 (best 1.06901), saving model to \"/dli/task/nemo_experiments/TextClassification/2024-03-12_11-21-15/checkpoints/TextClassification--val_loss=1.07-epoch=2.ckpt\" as top 3\n",
      "Saving latest checkpoint...\n",
      "Epoch 2: 100%|█| 13/13 [00:07<00:00,  1.74it/s, loss=0.969, v_num=1-15, lr=6.67e\n",
      "[NeMo W 2024-03-12 11:22:00 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:308: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      conf.update_node(conf_path, item.path)\n",
      "    \n",
      "[NeMo I 2024-03-12 11:22:22 text_classification_with_bert:121] Training finished!\n",
      "[NeMo I 2024-03-12 11:22:22 text_classification_with_bert:122] ===========================================================================================\n",
      "[NeMo I 2024-03-12 11:22:44 text_classification_with_bert:127] Model is saved into `.nemo` file: text_classification_model.nemo\n",
      "[NeMo I 2024-03-12 11:22:44 text_classification_with_bert:131] ===========================================================================================\n",
      "[NeMo I 2024-03-12 11:22:44 text_classification_with_bert:132] Starting the testing of the trained model on test set...\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Testing: 0it [00:00, ?it/s][NeMo I 2024-03-12 11:22:44 text_classification_model:165] test_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                              0.00       0.00       0.00          4\n",
      "    label_id: 1                                              0.00       0.00       0.00          2\n",
      "    label_id: 2                                             40.00     100.00      57.14          4\n",
      "    -------------------\n",
      "    micro avg                                               40.00      40.00      40.00         10\n",
      "    macro avg                                               13.33      33.33      19.05         10\n",
      "    weighted avg                                            16.00      40.00      22.86         10\n",
      "    \n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00,  7.41it/s]\n",
      "[NeMo I 2024-03-12 11:22:44 text_classification_with_bert:134] Testing finished!\n",
      "[NeMo I 2024-03-12 11:22:44 text_classification_with_bert:135] ===========================================================================================\n",
      "[NeMo I 2024-03-12 11:22:44 text_classification_with_bert:139] ===========================================================================================\n",
      "[NeMo I 2024-03-12 11:22:44 text_classification_with_bert:140] Starting the inference on some sample queries...\n",
      "[NeMo I 2024-03-12 11:22:44 text_classification_with_bert:144] The prediction results of some sample queries with the trained model:\n",
      "[NeMo I 2024-03-12 11:22:44 text_classification_with_bert:146] Query : In contrast no mutations were detected in the p53 gene suggesting that this tumour suppressor is not frequently altered in this leukaemia\n",
      "[NeMo I 2024-03-12 11:22:44 text_classification_with_bert:147] Predicted label: 2\n",
      "[NeMo I 2024-03-12 11:22:44 text_classification_with_bert:146] Query : The first predictive testing for Huntington disease  was based on analysis of linked polymorphic DNA markers to estimate the likelihood of inheriting the mutation for HD\n",
      "[NeMo I 2024-03-12 11:22:44 text_classification_with_bert:147] Predicted label: 2\n",
      "[NeMo I 2024-03-12 11:22:44 text_classification_with_bert:146] Query : Further studies suggested that low dilutions of C5D serum contain a factor or factors interfering at some step in the hemolytic assay of C5 rather than a true C5 inhibitor or inactivator\n",
      "[NeMo I 2024-03-12 11:22:44 text_classification_with_bert:147] Predicted label: 2\n",
      "[NeMo I 2024-03-12 11:22:44 text_classification_with_bert:149] Inference finished!\n",
      "[NeMo I 2024-03-12 11:22:44 text_classification_with_bert:150] ===========================================================================================\n",
      "CPU times: user 862 ms, sys: 393 ms, total: 1.25 s\n",
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The training takes about 3 minutes to run\n",
    "\n",
    "TC_DIR = \"/dli/task/nemo/examples/nlp/text_classification\"\n",
    "\n",
    "# set the values we want to override\n",
    "NUM_CLASSES = 3\n",
    "MAX_SEQ_LENGTH = 128\n",
    "PATH_TO_TRAIN_FILE = \"/dli/task/data/NCBI_tc-3/train_nemo_format.tsv\"\n",
    "PATH_TO_VAL_FILE = \"/dli/task/data/NCBI_tc-3/dev_nemo_format.tsv\"\n",
    "PATH_TO_TEST_FILE = \"/dli/task/data/NCBI_tc-3/test_nemo_format.tsv\"\n",
    "# disease domain inference sample answers should be 0, 1, 2 \n",
    "INFER_SAMPLES_0 = \"In contrast no mutations were detected in the p53 gene suggesting that this tumour suppressor is not frequently altered in this leukaemia \"\n",
    "INFER_SAMPLES_1 = \"The first predictive testing for Huntington disease  was based on analysis of linked polymorphic DNA markers to estimate the likelihood of inheriting the mutation for HD\"\n",
    "INFER_SAMPLES_2 = \"Further studies suggested that low dilutions of C5D serum contain a factor or factors interfering at some step in the hemolytic assay of C5 rather than a true C5 inhibitor or inactivator\"\n",
    "MAX_EPOCHS = 3\n",
    "\n",
    "# Run the training script, overriding the config values in the command line\n",
    "!python $TC_DIR/text_classification_with_bert.py \\\n",
    "        model.dataset.num_classes=$NUM_CLASSES \\\n",
    "        model.dataset.max_seq_length=$MAX_SEQ_LENGTH \\\n",
    "        model.train_ds.file_path=$PATH_TO_TRAIN_FILE \\\n",
    "        model.validation_ds.file_path=$PATH_TO_VAL_FILE \\\n",
    "        model.test_ds.file_path=$PATH_TO_TEST_FILE \\\n",
    "        model.infer_samples=[\"$INFER_SAMPLES_0\",\"$INFER_SAMPLES_1\",\"$INFER_SAMPLES_2\"] \\\n",
    "        trainer.max_epochs=$MAX_EPOCHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the start of each training experiment, there is a printed log of the experiment specification including any parameters added or overridden via the command-line. It also shows additional information, such as which GPUs are available, where logs are saved, and some samples from the datasets with their corresponding inputs to the model. The log also provides some stats on the lengths of sequences in the dataset.\n",
    "\n",
    "After each epoch, there is a summary table of metrics on the validation set which includes precision, recall, and f1 score. The f1 score takes both false positives and false negatives into account and is considered more useful than simple accuracy. \n",
    "\n",
    "At the end of training, NeMo saves the last checkpoint at the path specified by `model.nemo_file_path`.  Since we left this value at the default, it should have been written to our workspace in `.nemo` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_classification_model.nemo\n"
     ]
    }
   ],
   "source": [
    "!ls *.nemo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results you achieved in the experiment may not have been very good.  However, it is pretty easy to try another experiment with just a few changes.  Longer training, adjusted learning rate, and changing the batch size for the training and validation datasets can improve results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.4 Exercise: Run an Experiment\n",
    "Try running another similar experiment using the same text classification problem, this time with some suggested improvements:\n",
    "  \n",
    "* Set the mixed-precision `amp_level` to \"O1\" with a `precision` of 16 to make the model run faster with little or no reduction in accuracy.\n",
    "* Adjust the number of epochs upward a bit to improve results (larger `max_epochs`)\n",
    "* Increase the learning rate a little to allow more rapid response to the estimated error each time the model weights are updated\n",
    "\n",
    "The new values have been provided for you in the cell below.  Add the command with appropriate overrides and run the cell.  If you get stuck, refer to the [solution](solutions/ex2.2.4.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
      "Wall time: 7.15 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The training takes about 3 minutes to run\n",
    "\n",
    "TC_DIR = \"/dli/task/nemo/examples/nlp/text_classification\"\n",
    "\n",
    "# set the values we want to override\n",
    "NUM_CLASSES = 3\n",
    "MAX_SEQ_LENGTH = 128\n",
    "PATH_TO_TRAIN_FILE = \"/dli/task/data/NCBI_tc-3/train_nemo_format.tsv\"\n",
    "PATH_TO_VAL_FILE = \"/dli/task/data/NCBI_tc-3/dev_nemo_format.tsv\"\n",
    "PATH_TO_TEST_FILE = \"/dli/task/data/NCBI_tc-3/test_nemo_format.tsv\"\n",
    "# disease domain inference sample answers should be 0, 1, 2 \n",
    "INFER_SAMPLES_0 = \"In contrast no mutations were detected in the p53 gene suggesting that this tumour suppressor is not frequently altered in this leukaemia \"\n",
    "INFER_SAMPLES_1 = \"The first predictive testing for Huntington disease  was based on analysis of linked polymorphic DNA markers to estimate the likelihood of inheriting the mutation for HD\"\n",
    "INFER_SAMPLES_2 = \"Further studies suggested that low dilutions of C5D serum contain a factor or factors interfering at some step in the hemolytic assay of C5 rather than a true C5 inhibitor or inactivator\"\n",
    "MAX_EPOCHS = 5\n",
    "AMP_LEVEL = 'O1'\n",
    "PRECISION = 16\n",
    "LR = 5.0e-05\n",
    "\n",
    "# Override the config values in the command line\n",
    "# FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did the result from this experiment compare to the previous one?  Check the F1 scores and inference results in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.5 Visualize the Results with TensorBoard\n",
    "The [experiment manager](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/core/core.html?highlight=tensorboard#experiment-manager) saves results for viewing with TensorBoard. <br>\n",
    "Take a look by opening [TensorBoard](/tensorboard/) for your instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "To compare the performance of the models you've run, select the \"train loss\" scaler.  You can see all the models you've run compared together or select individual models for comparison.  The example below shows the first experiment in orange and the exercise experiment in blue.  You can see that the loss was smaller in the second experiment.\n",
    "\n",
    "<img src=\"images/tensorboard_01.png\" width=1000px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.6 Exercise: Change the Language Model\n",
    "So far, you've used the basic `bert-base-uncased` language model, but that is just one of many you could try.  Run the following cell to see what language models are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['megatron-bert-345m-uncased',\n",
       " 'megatron-bert-345m-cased',\n",
       " 'megatron-bert-uncased',\n",
       " 'megatron-bert-cased',\n",
       " 'biomegatron-bert-345m-uncased',\n",
       " 'biomegatron-bert-345m-cased',\n",
       " 'bert-base-uncased',\n",
       " 'bert-large-uncased',\n",
       " 'bert-base-cased',\n",
       " 'bert-large-cased',\n",
       " 'bert-base-multilingual-uncased',\n",
       " 'bert-base-multilingual-cased',\n",
       " 'bert-base-chinese',\n",
       " 'bert-base-german-cased',\n",
       " 'bert-large-uncased-whole-word-masking',\n",
       " 'bert-large-cased-whole-word-masking',\n",
       " 'bert-large-uncased-whole-word-masking-finetuned-squad',\n",
       " 'bert-large-cased-whole-word-masking-finetuned-squad',\n",
       " 'bert-base-cased-finetuned-mrpc',\n",
       " 'bert-base-german-dbmdz-cased',\n",
       " 'bert-base-german-dbmdz-uncased',\n",
       " 'cl-tohoku/bert-base-japanese',\n",
       " 'cl-tohoku/bert-base-japanese-whole-word-masking',\n",
       " 'cl-tohoku/bert-base-japanese-char',\n",
       " 'cl-tohoku/bert-base-japanese-char-whole-word-masking',\n",
       " 'TurkuNLP/bert-base-finnish-cased-v1',\n",
       " 'TurkuNLP/bert-base-finnish-uncased-v1',\n",
       " 'wietsedv/bert-base-dutch-cased',\n",
       " 'distilbert-base-uncased',\n",
       " 'distilbert-base-uncased-distilled-squad',\n",
       " 'distilbert-base-cased',\n",
       " 'distilbert-base-cased-distilled-squad',\n",
       " 'distilbert-base-german-cased',\n",
       " 'distilbert-base-multilingual-cased',\n",
       " 'distilbert-base-uncased-finetuned-sst-2-english',\n",
       " 'roberta-base',\n",
       " 'roberta-large',\n",
       " 'roberta-large-mnli',\n",
       " 'distilroberta-base',\n",
       " 'roberta-base-openai-detector',\n",
       " 'roberta-large-openai-detector',\n",
       " 'albert-base-v1',\n",
       " 'albert-large-v1',\n",
       " 'albert-xlarge-v1',\n",
       " 'albert-xxlarge-v1',\n",
       " 'albert-base-v2',\n",
       " 'albert-large-v2',\n",
       " 'albert-xlarge-v2',\n",
       " 'albert-xxlarge-v2']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# complete list of supported BERT-like models\n",
    "from nemo.collections import nlp as nemo_nlp\n",
    "nemo_nlp.modules.get_pretrained_lm_models_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, choose a new language model, such as `megatron-bert-345m-cased`.  \n",
    "\n",
    "You may need to restart the notebook kernal to clear memory.  If you use a large model, other ways to save GPU memory space are to reduce the `batch_size` to 32, 16, or even 8 and reduce the `max_seq_length` to 64. There is no right answer to this exercise.  Rather, this is an opportunity for you to experiment.  Some of the models can take several minutes to run, so feel free to move on to the next notebook and return here when you have time later. If you get stuck, take a look at an example [solution](solutions/ex2.2.6.ipynb).  Be sure to take note of the loss and f1 results with this model, or check TensorBoard for a visualization of the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the kernel\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO Try your own experiment with a different language model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2.3 PyTorch Lightning Model and Trainer Workflow\n",
    "The NeMo model script is the quickest way to get up and running.  Sometimes, though, you may want to create your own script or work through your project in a more customized manner.  In that case, you can step through the PyTorch Lightning workflow, which is otherwise abstracted (encapsulated and hidden) within the model training script. We'll take a look at the script, then try working through the same workflow from scratch without the script or Hydra.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.1 Script Key Features\n",
    "You can open the [text_classification_with_bert.py](nemo/examples/nlp/text_classification/text_classification_with_bert.py) script to see exactly what is happening.  \n",
    "\n",
    "Here's an abbreviated version with logging and initial comments removed:\n",
    "\n",
    "```python\n",
    "import pytorch_lightning as pl\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "from nemo.collections.nlp.models.text_classification import TextClassificationModel\n",
    "from nemo.collections.nlp.parts.nlp_overrides import NLPDDPPlugin\n",
    "from nemo.core.config import hydra_runner\n",
    "from nemo.utils.exp_manager import exp_manager\n",
    "\n",
    "\n",
    "@hydra_runner(config_path=\"conf\", config_name=\"text_classification_config\")\n",
    "def main(cfg: DictConfig) -> None:\n",
    "    trainer = pl.Trainer(plugins=[NLPDDPPlugin()], **cfg.trainer)\n",
    "    exp_manager(trainer, cfg.get(\"exp_manager\", None))\n",
    "\n",
    "    if not cfg.model.train_ds.file_path:\n",
    "        raise ValueError(\"'train_ds.file_path' need to be set for the training!\")\n",
    "\n",
    "    model = TextClassificationModel(cfg.model, trainer=trainer)\n",
    "    trainer.fit(model)\n",
    "\n",
    "    if cfg.model.nemo_path:\n",
    "        # '.nemo' file contains the last checkpoint and the params to initialize the model\n",
    "        model.save_to(cfg.model.nemo_path)\n",
    "\n",
    "    # We evaluate the trained model on the test set if test_ds is set in the config file\n",
    "    if cfg.model.test_ds.file_path:\n",
    "        trainer.test(model=model, ckpt_path=None, verbose=False)\n",
    "\n",
    "    # perform inference on a list of queries.\n",
    "    if \"infer_samples\" in cfg.model and cfg.model.infer_samples:       \n",
    "        # max_seq_length=512 is the maximum length BERT supports.\n",
    "        results = model.classifytext(queries=cfg.model.infer_samples, batch_size=16, max_seq_length=512)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```\n",
    "The Hydra decorator, `@hydra_runner`, connects the configuration file and provides the mechanism for the command line overrides. \n",
    "\n",
    "Once the configuration is established, the key steps are:\n",
    "1. Instantiate the trainer with `trainer = pl.Trainer(plugins=[NLPDDPPlugin()], **cfg.trainer)`\n",
    "1. Instantiate the model with `model = TextClassificationModel(cfg.model, trainer=trainer)`\n",
    "1. Train the model with `trainer.fit(model)`\n",
    "\n",
    "Additional steps for optional inference and evaluation are:\n",
    "* Evaluate with `trainer.test(model=model, ckpt_path=None, verbose=False)`\n",
    "* Infer with `results = model.classifytext(queries=cfg.model.infer_samples, batch_size=16, max_seq_length=512)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.3.2 Model Training from Scratch\n",
    "Execute the following cell to restart the notebook kernel to clear variables and GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the kernel\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the `nemo_nlp` collection, the experiment manager, PyTorch Lightning, and OmegaConf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections import nlp as nemo_nlp\n",
    "from nemo.utils.exp_manager import exp_manager\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running these training steps manually without the script or Hydra, the correct configuration must be set prior to instantiation.  We've already determined the changes we want to make to the config file for this project.  Previously, we used the Hydra override feature in the command line, but those changes are made this time using `OmegaConf`. The syntax looks similar, except this time we are directly changing the `OmegaConf` object, `config`, in Python, and will pass that object to `trainer`, `exp_manager`, and `model`.\n",
    "\n",
    "The default language model is `bert-base-uncased`.  To override it, add to the cell (for example):\n",
    "```python\n",
    "    PRETRAINED_MODEL_NAME = 'bert-base-cased'\n",
    "    config.model.language_model.pretrained_model_name=PRETRAINED_MODEL_NAME\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the OmegaConf object by loading the config file\n",
    "TC_DIR = \"/dli/task/nemo/examples/nlp/text_classification\"\n",
    "CONFIG_FILE = \"text_classification_config.yaml\"\n",
    "config = OmegaConf.load(TC_DIR + \"/conf/\" + CONFIG_FILE)\n",
    "\n",
    "# set the values we want to change\n",
    "NUM_CLASSES = 3\n",
    "MAX_SEQ_LENGTH = 128\n",
    "PATH_TO_TRAIN_FILE = \"/dli/task/data/NCBI_tc-3/train_nemo_format.tsv\"\n",
    "PATH_TO_VAL_FILE = \"/dli/task/data/NCBI_tc-3/dev_nemo_format.tsv\"\n",
    "PATH_TO_TEST_FILE = \"/dli/task/data/NCBI_tc-3/test_nemo_format.tsv\"\n",
    "# disease domain inference sample answers should be 0, 1, 2 \n",
    "INFER_SAMPLES = [\"Germline mutations in BRCA1 are responsible for most cases of inherited breast and ovarian cancer \",\n",
    "        \"The first predictive testing for Huntington disease  was based on analysis of linked polymorphic DNA markers to estimate the likelihood of inheriting the mutation for HD\",\n",
    "        \"Further studies suggested that low dilutions of C5D serum contain a factor or factors interfering at some step in the hemolytic assay of C5 rather than a true C5 inhibitor or inactivator\"\n",
    "        ]\n",
    "MAX_EPOCHS = 5\n",
    "AMP_LEVEL = 'O1'\n",
    "PRECISION = 16\n",
    "LR = 5.0e-05\n",
    "\n",
    "# set the config values using omegaconf\n",
    "config.model.dataset.num_classes = NUM_CLASSES\n",
    "config.model.dataset.max_seq_length = MAX_SEQ_LENGTH\n",
    "config.model.train_ds.file_path = PATH_TO_TRAIN_FILE\n",
    "config.model.validation_ds.file_path = PATH_TO_VAL_FILE\n",
    "config.model.test_ds.file_path = PATH_TO_TEST_FILE\n",
    "config.model.infer_samples = INFER_SAMPLES\n",
    "config.trainer.max_epochs = MAX_EPOCHS\n",
    "config.trainer.amp_level = AMP_LEVEL\n",
    "config.trainer.precision = PRECISION\n",
    "config.model.optim.lr = LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that `config` has been updated with the correct values, instantiate the trainer and experiment manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the trainer and experiment manager\n",
    "trainer = pl.Trainer(**config.trainer)\n",
    "exp_manager(trainer, config.exp_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the model \n",
    "model = nemo_nlp.models.TextClassificationModel(config.model, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# start model training and save result\n",
    "# The training takes about 3 minutes to run\n",
    "trainer.fit(model)\n",
    "model.save_to(config.model.nemo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with `trainer.test`, which will automatically use the file path to the test set we updated in `config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model=model, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run inference using the inference samples from `config`. We can check on them by just printing directly from the `config.model.infer_samples` key object.  It displays as a list of strings.\n",
    "\n",
    "To run inference tor text classification, use the `model.classifytext` method.  The inferred labels are output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(config.model.infer_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifytext(queries=config.model.infer_samples, batch_size=64, max_seq_length=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.3 Exercise: Query the Model\n",
    "What if we wanted to specify additional queries for inference?  The `model.classifytext` method we just used specifies the queries, but they do not _have_ to be in the config file.  We can simply create a list of strings for our queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_queries = [\n",
    "    'Occasional missense mutations in ATM were also found in tumour DNA from patients with B-cell non-Hodgkins lymphomas.',\n",
    "    'Myotonic dystrophy protein kinase is involved in the modulation of the Ca2+ homeostasis in skeletal muscle cells.',\n",
    "    'Constitutional RB1-gene mutations in patients with isolated unilateral retinoblastoma.',\n",
    "    'Hereditary deficiency of the fifth component of complement in man. I. Clinical, immunochemical, and family studies.'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference on the `my_queries` list.  If you get stuck, refer to the [solution](solutions/ex2.3.3.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Run inference over the my_queries list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "You've built a text classifier with three classes and learned:\n",
    "* How to use NeMo NLP model config files and scripts to quickly create experiments\n",
    "* How to override the config `model`, `trainer`, and `exp_manager settings`\n",
    "* How to train, evaluate, and infer a text classifier using a single command line\n",
    "* How to train, evaluate, and infer a text classifier using PyTorch Lightning\n",
    "\n",
    "You're ready to try a different NLP task.<br>\n",
    "\n",
    "Move on to [3.0 Build a Named Entity Recognizer](030_NamedEntityRecognition.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
